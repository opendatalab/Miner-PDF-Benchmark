# Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments 

Junoh Heo and Chih-Li Sung*<br>Michigan State University


#### Abstract

Computer simulations have become essential for analyzing complex systems, but high-fidelity simulations often come with significant computational costs. To tackle this challenge, multi-fidelity computer experiments have emerged as a promising approach that leverages both low-fidelity and high-fidelity simulations, enhancing both the accuracy and efficiency of the analysis. In this paper, we introduce a new and flexible statistical model, the Recursive Non-Additive (RNA) emulator, that integrates the data from multi-fidelity computer experiments. Unlike conventional multi-fidelity emulation approaches that rely on an additive auto-regressive structure, the proposed RNA emulator recursively captures the relationships between multi-fidelity data using Gaussian process priors without making the additive assumption, allowing the model to accommodate more complex data patterns. Importantly, we derive the posterior predictive mean and variance of the emulator, which can be efficiently computed in a closed-form manner, leading to significant improvements in computational efficiency. Additionally, based on this emulator, we introduce four active learning strategies that optimize the balance between accuracy and simulation costs to guide the selection of the fidelity level and input locations for the next simulation run. We demonstrate the effectiveness of the proposed approach in a suite of synthetic examples and a real-world problem. An R package RNAmf for the proposed methodology is provided on CRAN.


Keywords: Surrogate model; Sequential design; Uncertainty quantification; Gaussian process; Auto-regressive model.[^0]

## 1 Introduction

Computer simulations play a crucial role in engineering and scientific research, serving as valuable tools for predicting the performance of complex systems across diverse fields such as aerospace engineering (Mak et al., 2018), natural disaster prediction (Ma et al., 2022), and cell biology (Sung et al., 2020). However, conducting high-fidelity simulations for parameter space exploration can be demanding due to prohibitive costs and timeconsuming computations. To address this challenge, multi-fidelity emulation has emerged as a promising alternative. It leverages computationally expensive yet accurate high-fidelity simulations alongside computationally inexpensive but potentially less accurate low-fidelity simulations to create an efficient predictive model, emulating the expensive computer code. By strategically integrating these simulations and designing multi-fidelity experiments, we can potentially improve accuracy without excessive computational resources.

The usefulness of the multi-fidelity emulation framework has driven extensive research in recent years. One popular approach is the Kennedy-O'Hagan (KO) model (Kennedy and O'Hagan, 2000), which models a sequence of computer simulations from lowest to highest fidelity using a sequence of Gaussian process (GP) models (Gramacy, 2020; Rasmussen and Williams, 2006), linked by a linear auto-regressive framework. This model has made significant contributions across various fields employing multi-fidelity computer experiments (see, e.g., Patra et al. (2020), Kuya et al. (2011), and Demeyer et al. (2017)), and several recent developments, including Qian et al. (2006), Le Gratiet (2013), Le Gratiet and Garnier (2014), Qian and Wu (2008), Perdikaris et al. (2017), and Ji et al. (2024) (among many others), have investigated modeling strategies for efficient posterior prediction and Bayesian uncertainty quantification.

Despite this body of work, most of these approaches rely on the assumption of linear correlation between low-fidelity and high-fidelity data, resulting in an additive GP structure. With the growing complexity of modern data, such models face challenges in capturing complex relationships between data with different fidelity levels. As shown in the left panel of Figure 1, where the relationship between high-fidelity data and low-fidelity data is nonlinear, the KO model falls short in providing accurate predictions due to its limited



Figure 1: An example adapted from Perdikaris et al. (2017), where $n_{1}=13$ samples (red dots) are collected from the low-fidelity simulator $f_{1}(x)=\sin (8 \pi x)$ (red dashed line), and $n_{2}=8$ samples (black triangles) are collected from the high-fidelity simulator $f_{2}(x)=(x-\sqrt{2}) f_{1}^{2}(x)$ (black solid line). The KO emulator (left panel) and the RNA emulator (right panel) are shown as blue lines. Gray shaded regions represent the $95 \%$ pointwise confidence intervals.

flexibility resulting from the additive structure it assumes.

In this paper, we propose a new and flexible model that captures the nonlinear relationships between multi-fidelity data in a recursive manner. This flexible nonlinear functional form can encompass many existing models, including the $\mathrm{KO}$ model, as a special case. Specifically, instead of assuming a specific form for the relationship, our model uses GP priors to model the relationship between multi-fidelity data, resulting in a non-additive GP model. Hence, we refer to this proposed method as the Recursive Non-Additive (RNA) emulator. As shown in the right panel of Figure 1, the RNA emulator demonstrates its superior ability to the KO model by emulating the high-fidelity simulator with high accuracy and low uncertainty, showcasing its flexibility and power of our nonlinear modeling approach.

The RNA emulator belongs to the emerging field of linked/deep GP models (see, e.g., Kyzyurova et al. (2018), Ming and Guillas (2021), Sauer et al. (2023) and Ming et al. (2023)), where different GPs are connected in a coupled manner. To the best of our knowledge, there has been limited research on extending such results for the analysis of multi-fidelity
computer experiments, and we aim to address this gap in our work. Notably, recent work by Perdikaris et al. (2017) has made progress in this direction, but their approach assumes an additive structure for the kernel function, and employs the Monte Carlo integration to handle intractable posterior distributions. Recent advancement by Ko and Kim (2022) extends deep GP models to address nonstationarity, but still roots in the additive structure of the KO model. Similarly, Cutajar et al. (2019) employs an additive kernel akin to Perdikaris et al. (2017) and relies on the sparse variational approximation for inference. In a similar vein, Meng and Karniadakis (2020), Li et al. (2020), Meng et al. (2021), and Kerleguer et al. (2024) establish connections between different fidelities using (Bayesian) neural networks. In contrast, our proposed model not only provides great flexibility using GP priors with commonly used kernel structures to connect multi-fidelity data, but also provides analytical expressions for both the posterior mean and variance, enabling the expectation propagation method to approximate the intractable posterior distribution. This computational improvement allows for more efficient calculations, facilitating efficient uncertainty quantification.

Leveraging this newly developed RNA emulator, we introduce four active learning strategies to achieve enhanced accuracy while carefully managing the limited simulation resources, particularly crucial for computationally expensive simulations. Active learning, also known as sequential design, involves sequentially searching for and acquiring new data points at optimal locations based on a given sampling criterion, to construct an accurate surrogate model/emulator. While active learning has been well-established for single-fidelity GP emulators (Gramacy, 2020 Rasmussen and Williams, 2006; Santner et al. 2018), research in the context of multi-fidelity computer experiments is scarce and more challenging. This is because it requires simultaneous selection of optimal input locations and fidelity levels, accounting for their respective simulation costs. Although some recent works have considered cost in specific cases like single-objective unconstrained optimization (Huang et al., 2006; Swersky et al., 2013; He et al., 2017) and global approximation (Xiong et al., 2013, Le Gratiet and Cannamela, 2015; Stroh et al., 2022, Ehara and Guillas, 2023 , Sung et al., 2024a), most of these works are developed based on the KO model, and the
active learning for non-additive GP models has not been fully explored in the literature. In addition, popular sampling criteria for global approximation, such as "Active Learning MacKay" (McKay et al. (2000), ALM) and "Active Learning Cohn" (Cohn (1993), ALC), remain largely unexplored in the context of multi-fidelity computer experiments. Recent successful applications of these sampling criteria to other learning problems can be found in Binois et al. (2019), Park et al. (2023), Koermer et al. (2023), and Sauer et al. (2023).

Our main contribution lies in advancing active learning with these popular sampling criteria, based on this newly developed RNA emulator. This advancement enables the optimal selection of input locations and fidelity levels while considering the associated costs, resulting in improved predictability for learning the nonlinear relationships between multi-fidelity simulators. It is important to note that few existing works in the multi-fidelity deep GP literature (Perdikaris et al., 2017; Cutajar et al., 2019; Ko and Kim, 2022) delve into active learning, mainly due to computational complexities associated with computing acquisition functions. In contrast, our closed-form posterior mean and variance of the RNA emulator not only facilitate efficient computation of these sampling criteria, supporting active learning efforts, but also provide valuable mathematical insights into the active learning. In particular, we introduce a sampling criterion based on the decomposition of the posterior variance. To facilitate broader usage and ensure reproducibility, we implement an $\mathrm{R}(\mathrm{R}$ Core Team, 2018) package called RNAmf, which is available in a public repository.

The structure of this article is as follows. In Section 2, we provide a brief review of the KO model. Our proposed RNA emulator is introduced in Section 3. Section 4 outlines our active learning strategies based on the RNA emulator. Numerical and real data studies are presented in Sections 5 and 6, respectively. Lastly, we conclude the paper in Section 7 .

## 2 Background

In this section, we provide an overview of the background information regarding multi-fidelity emulation.

### 2.1 Problem Setup

Let $f_{l}(\mathbf{x})$ represent the scalar simulation output of the computer code with input parameters $\mathbf{x} \in \Omega \subseteq \mathbb{R}^{d}$ at fidelity level $l=1, \ldots, L$. Throughout this paper, we assume that $L$ distinct fidelity levels of simulations are conducted for training an emulator, where a higher fidelity level corresponds to a simulator with more accurate outputs but also higher simulation costs per run.

Our primary objective is to construct an efficient emulator for the highest-fidelity and most expensive simulation code, $f_{L}(\mathbf{x})$. For each fidelity level $l$, we perform simulations

at $n_{l}$ design points denoted by $\mathcal{X}_{l}=\left\{\mathbf{x}_{i}^{[l]}\right\}_{i=1}^{n_{l}}$. These simulations yield the corresponding simulation outputs $\mathbf{y}_{l}:=\left(f_{l}(\mathbf{x})\right)_{\mathbf{x} \in \mathcal{X}_{l}}$, representing the vector of outputs for $f_{l}(\mathbf{x})$ at design

points $\mathbf{x} \in \mathcal{X}_{l}$, and each element of $\mathbf{y}_{l}$ is denoted by $y_{i}^{[l]}=f_{l}\left(\mathbf{x}_{i}^{[l]}\right)$. We assume that the designs $\mathcal{X}_{l}$ are sequentially nested, i.e.,

$$
\mathcal{X}_{L} \subseteq \mathcal{X}_{L-1} \subseteq \cdots \subseteq \mathcal{X}_{1} \subseteq \Omega
$$

and $\mathbf{x}_{i}^{[l]}=\mathbf{x}_{i}^{[l-1]}$ for $i=1, \ldots, n_{l}$. In other words, design points run for a higher fidelity simulator are contained within the design points run for a lower fidelity simulator. This property has been shown to lead to more efficient inference in various multi-fidelity emulation approaches (Qian, 2009; Qian et al., 2009; Haaland and Qian, 2010).

Furthermore, we let $C_{l}$ denote the simulation cost (e.g., in CPU hours) for a single run of the simulator at fidelity level $l$. Since higher-fidelity simulators are more computationally demanding, this implies that $0<C_{1}<C_{2}<\ldots<C_{L}$.

### 2.2 Auto-regressive model

One of the prominent approaches for modeling $f_{L}(\mathbf{x})$ is the auto-regressive model proposed by Kennedy and O'Hagan (2000), referred to as the KO model hereafter, which can be
expressed as follows:

$$
\left\{\begin{array}{l}
f_{1}(\mathbf{x})=Z_{1}(\mathbf{x}) \\
f_{l}(\mathbf{x})=\rho_{l-1} f_{l-1}(\mathbf{x})+Z_{l}(\mathbf{x}), \quad \text { for } \quad 2 \leq l \leq L
\end{array}\right.
$$

where $\rho_{l-1}$ is an unknown auto-regressive parameter, and $Z_{l}=\left(f_{l}-\rho_{l-1} f_{l-1}\right)$ represents the discrepancy between the $(l-1)$-th and $l$-th code. The KO model considers a probabilistic surrogate model by assuming that $\left\{Z_{l}\right\}_{l=1}^{L}$ follow independent zero-mean Gaussian process (GP) models:

$$
Z_{l}(\mathbf{x}) \stackrel{\text { indep. }}{\sim} \mathcal{G} \mathcal{P}\left\{\alpha_{l}(\mathbf{x}), \tau_{l}^{2} \Phi_{l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right\}, \quad l=1, \ldots, L
$$

where $\alpha_{l}(\mathbf{x})$ is a mean function, $\tau_{l}^{2}$ is a variance parameter, and $\Phi_{l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ is a positive-definite kernel function defined on $\Omega \times \Omega$. In the original KO paper, $\alpha_{l}(\mathbf{x})$ is assumed to be $h(\mathbf{x}) \beta_{l}$, where $h(\mathbf{x})$ is a vector of $d$ regression functions. Other common choices for $\alpha_{l}(\mathbf{x})$ include $\alpha_{l}(\mathbf{x}) \equiv 0$ or $\alpha_{l}(\mathbf{x}) \equiv \mu_{l}$. As for the kernel function, popular choices include the squared exponential kernel and Matérn kernel (Stein, 1999). Specifically, the anisotropic squared exponential kernel takes the form:

$$
\Phi_{l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\prod_{j=1}^{d} \phi\left(x_{j}, x_{j}^{\prime} ; \theta_{l j}\right)=\prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{j}^{\prime}\right)^{2}}{\theta_{l j}}\right)
$$

where $\left(\theta_{l 1}, \ldots, \theta_{l d}\right)$ is the lengthscale hyperparameter, indicating that the correlation decays exponentially fast in the squared distance between $\mathbf{x}$ and $\mathbf{x}^{\prime}$. The GP model (3), combined with the auto-regressive model (2), implies that, conditional on the parameters $\tau_{l}^{2}, \theta_{l j}$, and $\mu(\cdot)$, the joint distribution of $\left(\mathbf{y}_{1}, \ldots, \mathbf{y}_{L}\right)$ follows a multivariate normal distribution, and these unknown parameters can be estimated via maximum likelihood estimation or Bayesian inference. Given the data $\left(\mathbf{y}_{1}, \ldots, \mathbf{y}_{L}\right)$, it can be shown that the posterior distribution of $f_{L}(\mathbf{x})$ is also a GP. The posterior mean function can then be used to emulate the expensive simulator, while the posterior variance function can be employed to quantify the emulation uncertainty. We refer the details to Kennedy and O'Hagan (2000).

The auto-regressive framework of the $\mathrm{KO}$ model has led to the development of several
variants. For instance, Le Gratiet and Garnier (2014) introduce a faster algorithm based on a recursive formulation for computing the posterior of $f_{l}(\mathbf{x})$ more efficiently. To enhance the model's flexibility, Qian et al. (2006), Le Gratiet and Garnier (2014), and Qian and $\mathrm{Wu}(2008)$ allow the auto-regressive parameter $\rho_{l}$ to depend on the input $\mathbf{x}$, that is, $f_{l}(\mathbf{x})=\rho_{l-1}(\mathbf{x}) f_{l-1}(\mathbf{x})+Z_{l}(\mathbf{x})$ for $2 \leq l \leq L$, where the first two assume $\rho_{l-1}(\mathbf{x})$ to be a linear function, while the last one assume $\rho_{l-1}(\mathbf{x})$ to be a GP.

## 3 Recursive Non-Additive (RNA) emulator

Despite the advantages of the $\mathrm{KO}$ model, it results in an additive GP model based on (22) and (3), which may not adequately capture the nonlinear relationships between data at different fidelity levels. To overcome this limitation and achieve a more flexible representation, we propose a novel Recursive Non-Additive (RNA) emulator, defined in a recursive fashion:

$$
\left\{\begin{array}{l}
f_{1}(\mathbf{x})=W_{1}(\mathbf{x}) \\
f_{l}(\mathbf{x})=W_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right), \quad \text { for } \quad l=2, \ldots, L
\end{array}\right.
$$

The model structure is illustrated in Figure 2. This RNA model offers greater flexibility and can be viewed as many existing models as a special case. For instance, the auto-regressive $\mathrm{KO}$ model can be represented in the form of (5) by setting $W_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)=\rho_{l-1} f_{l-1}(\mathbf{x})+\delta(\mathbf{x})$, and it can also be considered as the model in Qian et al. (2006), Le Gratiet and Garnier (2014), and Qian and Wu (2008) by setting $W_{l}\left(x, f_{l-1}(\mathbf{x})\right)=\rho_{l-1}(\mathbf{x}) f_{l-1}(\mathbf{x})+\delta(\mathbf{x})$.

Instead of assuming a specific form for the relationship, we model the relationship $W_{l}$ using a GP prior, that is, $W_{1}(\mathbf{x}) \sim \mathcal{G} \mathcal{P}\left\{\alpha_{1}(\mathbf{x}), \tau_{1}^{2} \Phi_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right\}$ and

$$
W_{l}(\mathbf{z}) \sim \mathcal{G} \mathcal{P}\left\{\alpha_{l}(\mathbf{z}), \tau_{l}^{2} K_{l}\left(\mathbf{z}, \mathbf{z}^{\prime}\right)\right\}, \quad l=2, \cdots, L
$$

where $\mathbf{z}=(\mathbf{x}, y)$, forming a vector of size $(d+1)$, and $K_{l}\left(\mathbf{z}, \mathbf{z}^{\prime}\right)$ is a positive-definite kernel. We consider a constant mean, i.e., $\alpha_{1}(\mathbf{x})=\alpha_{1}$ and $\alpha_{l}(\mathbf{z})=\alpha_{l}$ for $l \geq 2$. We adopt popular kernel choices for $K_{l}$, such as the squared exponential kernel and Matérn kernel. In



Figure 2: An illustration of the recursive structure of the RNA model.

particular, the squared exponential kernel $K_{l}$, following (4), can be expressed as:

$$
\begin{aligned}
K_{l}\left(\mathbf{z}, \mathbf{z}^{\prime}\right) & =\phi\left(y_{l-1}, y_{l-1}^{\prime} ; \theta_{l y}\right) \prod_{j=1}^{d} \phi\left(x_{j}, x_{j}^{\prime} ; \theta_{l j}\right) \\
& =\exp \left(-\frac{\left(y_{l-1}-y_{l-1}^{\prime}\right)^{2}}{\theta_{l y}}\right) \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{j}^{\prime}\right)^{2}}{\theta_{l j}}\right)
\end{aligned}
$$

where the lengthscale hyperparameter, $\boldsymbol{\theta}_{l}=\left(\theta_{l 1}, \ldots, \theta_{l d}, \theta_{l y}\right)$, represents a vector of size $(d+1)$. The Matérn kernel can be similarly constructed, which is given in the Appendix A.

Combining the GP model (6) with the recursive formulation (5) and assuming the nested design as defined in (1), the observed simulations $\mathbf{y}_{l}$ follow a multivariate normal distribution:

$$
\mathbf{y}_{l} \sim \mathcal{N}_{n_{l}}\left(\alpha_{l} \mathbf{1}_{n_{l}}, \tau_{l}^{2} \mathbf{K}_{l}\right) \quad \text { for } \quad l=1, \ldots, L
$$

where $\mathbf{1}_{n_{l}}$ is a unit vector of size $n_{l}, \mathbf{K}_{l}$ is an $n_{l} \times n_{l}$ matrix with each element $\left(\mathbf{K}_{l}\right)_{i j}=$ $\Phi_{1}\left(\mathbf{x}_{i}^{[1]}, \mathbf{x}_{j}^{[1]}\right)$ for $l=1$, and $\left(\mathbf{K}_{l}\right)_{i j}=K_{l}\left(\mathbf{z}_{i}^{[l]}, \mathbf{z}_{j}^{[l]}\right)$ for $l \geq 2$, where $\mathbf{z}_{i}^{[l]}=\left(\mathbf{x}_{i}^{[l]}, f_{l-1}\left(\mathbf{x}_{i}^{[l]}\right)\right)$. Note that $f_{l-1}\left(\mathbf{x}_{i}^{[l]}\right)=y_{i}^{[l-1]}$ due to the nested design assumption (1), which is the $i$-th simulation output at level $l-1$. The parameters $\left\{\alpha_{l}, \tau_{l}^{2}, \boldsymbol{\theta}_{l}\right\}_{l=1}^{L}$ can be estimated by maximum likelihood estimation. Specifically, the log-likelihood (up to an additive constant) is

$$
-\frac{1}{2}\left(n_{l} \log \left(\tau_{l}^{2}\right)+\log \left(\operatorname{det}\left(\mathbf{K}_{l}\right)\right)+\frac{1}{\tau_{l}^{2}}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)^{T} \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right)
$$

The parameters can then be efficiently estimated by maximizing the log-likelihood via an optimization algorithm, such as quasi-Newton optimization method of Byrd et al. (1995).

It is important to acknowledge that the idea of a recursive GP was previously proposed by Perdikaris et al. (2017), referred to as a nonlinear auto-regressive model therein. However, there are two key distinctions between their model and ours. The first distinction lies in the kernel assumption, where they assume an additive form of the kernel to better capture the auto-regressive nature. Specifically, they use $K_{l}\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=\Phi_{l 1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \Phi_{l 2}\left(f_{l-1}(\mathbf{x}), f_{l-1}\left(\mathbf{x}^{\prime}\right)\right)+$ $\Phi_{l 3}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ with valid kernel functions $\Phi_{l 1}, \Phi_{l 2}$, and $\Phi_{l 3}$. While our kernel function shares some similarities, particularly the first component $\Phi_{l 1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \Phi_{l 2}\left(f_{l-1}(\mathbf{x}), f_{l-1}\left(\mathbf{x}^{\prime}\right)\right)$, the role of the second component $\Phi_{l 3}$ in predictions remains unclear. The inclusion of this component introduces $d$ hyperparameters for an anisotropic kernel, making the estimation more challenging with limited samples, especially for high-dimensional problems. In contrast, we adopt the natural form of popular kernel choices, such as the squared exponential kernel in (7) and the Matérn kernel, placing our model within the emerging field of linked/deep GP models, which has shown promising results in the computer experiment literature (Kyzyurova et al., 2018 Ming and Guillas, 2021; Sauer et al., 2023). The second distinction is in the computation for the posterior of $f_{L}(\mathbf{x})$. Specifically, their model relies on Monte Carlo (MC) integration for their computation, which can be quite computationally demanding, especially in this recursive formulation. In contrast, with these popular kernel choices, we can derive the posterior mean and variance of $f_{L}(\mathbf{x})$ in a closed form, which is presented in the following proposition, enabling more efficient predictions and uncertainty quantification.

The derivation of the posterior follows these steps. Starting with the GP assumption (6), and utilizing the properties of conditional multivariate normal distribution, the posterior distribution of $f_{l}$ given $\mathbf{y}_{l}$ and $f_{l-1}$ at a new input location $\mathbf{x}$ is normally distributed, namely: $f_{1}(\mathbf{x}) \mid \mathbf{y}_{1} \sim \mathcal{N}\left(\mu_{1}(\mathbf{x}), \sigma_{1}^{2}(\mathbf{x})\right)$ with

$$
\begin{gathered}
\mu_{1}(\mathbf{x})=\alpha_{1} \mathbf{1}_{n_{1}}+\mathbf{k}_{1}^{T}(\mathbf{x}) \mathbf{K}_{1}^{-1}\left(\mathbf{y}_{1}-\alpha_{l} \mathbf{1}_{n_{1}}\right), \quad \text { and } \\
\sigma_{1}^{2}(\mathbf{x})=\tau_{1}^{2}\left(1-\mathbf{k}_{1}(\mathbf{x})^{T} \mathbf{K}_{1}^{-1} \mathbf{k}_{1}(\mathbf{x})\right)
\end{gathered}
$$

and $f_{l}(\mathbf{x}) \mid \mathbf{y}_{l}, f_{l-1}(\mathbf{x}) \sim \mathcal{N}\left(\mu_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right), \sigma_{l}^{2}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right)$ for $l=2, \ldots, L$ with

$$
\begin{aligned}
\mu_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) & =\alpha_{l} \mathbf{1}_{n_{l}}+\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right), \quad \text { and } \\
\sigma_{l}^{2}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) & =\tau_{l}^{2}\left(1-\mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)^{T} \mathbf{K}_{l}^{-1} \mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right)
\end{aligned}
$$

where $\mathbf{k}_{1}(\mathbf{x})$ and $\mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)$ are an $n_{l} \times 1$ matrix with each element $\left(\mathbf{k}_{1}(\mathbf{x})\right)_{i, 1}=\Phi_{l}\left(\mathbf{x}, \mathbf{x}_{i}^{[l]}\right)$ and $\left(\mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right)_{i, 1}=K_{l}\left(\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right),\left(\mathbf{x}_{i}^{[l]}, y_{i}^{[l-1]}\right)\right)$ for $l \geq 2$, respectively. The posterior distribution of $f_{l}$ can then be obtained by

$$
\begin{aligned}
& p\left(f_{l}(\mathbf{x}) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{l}\right) \\
= & \int \cdots \int p\left(f_{l}(\mathbf{x}) \mid \mathbf{y}_{l}, f_{l-1}(\mathbf{x})\right) p\left(f_{l-1}(\mathbf{x}) \mid \mathbf{y}_{l-1}, f_{l-2}(\mathbf{x})\right) \cdots p\left(f_{1}(\mathbf{x}) \mid \mathbf{y}_{1}\right) \mathrm{d}\left(f_{l-1}(\mathbf{x})\right) \ldots \mathrm{d}\left(f_{1}(\mathbf{x})\right)
\end{aligned}
$$

This posterior is analytically intractable but can be numerically approximated using MC integration, as done in Perdikaris et al. (2017), which involves sequential sampling from the normal distribution $p\left(f_{l}(\mathbf{x}) \mid \mathbf{y}_{l}, f_{l-1}(\mathbf{x})\right)$ from $l=1$ to $l=L$. However, this method can be computationally demanding and can become impractical for uncertainty quantification, especially when the dimensions of $\mathbf{x}$ and the number of fidelity levels increase. To address this, we derive recursive closed-form expressions for the posterior mean and variance under popular kernel choices as follows.

Proposition 3.1. Under the squared exponential kernel function (7), the posterior mean and variance of $f_{l}(\mathbf{x})$ given the data $\mathbf{y}_{1}, \ldots, \mathbf{y}_{l}$ for $l \geq 2$ can be expressed in a recursive fashion as follows,

$$
\begin{aligned}
\mu_{l}^{*}(\mathbf{x}): & =\mathbb{E}\left[f_{l}(\mathbf{x}) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{l}\right] \\
& =\alpha_{l}+\sum_{i=1}^{n_{l}} r_{i} \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{i j}^{[l]}\right)^{2}}{\theta_{l j}}\right) \frac{1}{\sqrt{1+2 \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{\theta_{l y}+2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)
\end{aligned}
$$

and

$$
\begin{aligned}
\sigma_{l}^{* 2}(\mathbf{x}):=\mathbb{V} & \left.f_{l}(\mathbf{x}) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{l}\right]=\tau_{l}^{2}-\left(\mu_{l}^{*}(\mathbf{x})-\alpha_{l}\right)^{2}+ \\
& \left(\sum_{i, k=1}^{n_{l}} \zeta_{i k}\left(r_{i} r_{k}-\tau_{l}^{2}\left(\mathbf{K}_{l}^{-1}\right)_{i k}\right) \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{i j}^{[l]}\right)^{2}+\left(x_{j}-x_{k j}^{[l]}\right)^{2}}{\theta_{l j}}\right)\right)
\end{aligned}
$$

where $r_{i}=\left(\mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right)_{i}$, and

$$
\zeta_{i k}=\frac{1}{\sqrt{1+4 \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}} \exp \left(-\frac{\left.\frac{y_{i}^{[l-1]}+y_{k}^{[l-1]}}{2}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{\frac{\theta_{l y}}{2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})}-\frac{\left(y_{i}^{[l-1]}-y_{k}^{[l-1]}\right)^{2}}{2 \theta_{l y}}\right)
$$

For $l=1$, it follows that $\mu_{1}^{*}(\mathbf{x})=\mu_{1}(\mathbf{x})$ and $\sigma_{1}^{* 2}(\mathbf{x})=\sigma_{1}^{2}(\mathbf{x})$ as in (8) and (9), respectively.

The posterior mean and variance under a Matérn kernel with the smoothness parameter $\nu=1.5$ and $\nu=2.5$ are provided in the Supplementary Materials S2, and the detailed derivations for Proposition 3.1 are provided in Supplementary Materials S1, which follow the proof of Kyzyurova et al. (2018) and Ming and Guillas (2021). With this proposition, the posterior mean and variance can be efficiently computed in a recursive fashion. Similar to Kyzyurova et al. (2018) and Ming and Guillas (2021), we adopt the moment matching method, using a Gaussian distribution to approximate the posterior distribution with the mean and variance presented in the proposition. The parameters in the posterior distribution, including $\left\{\alpha_{l}, \tau_{l}^{2}, \boldsymbol{\theta}_{l}\right\}_{l=1}^{L}$, can be plugged in by their estimates.

Notably, Proposition 3.1 can be viewed as a simplified representation of Theorem 3.3 from Ming and Guillas (2021) for constructing a linked GP surrogate. However, it is important to highlight the distinctions and contributions of our work, particularly in the context of multi-fidelity computer experiments. Firstly, there are currently no existing closed-form expressions for the posterior mean and variance in the multi-fidelity deep GP literature. By providing such expressions, our work fills this gap, offering valuable mathematical insights and enhancing computational efficiency for active learning strategies, which will be discussed in Section 3.1 and Section 4, respectively. Additionally, while the linked GP model provides a general framework, much of the discussion in their work focuses on sequential

GPs, where the output of the high-layer emulator depends solely on the output of the low-layer emulator, i.e., $W_{2}\left(W_{1}(x)\right)$. Our setup differs slightly, as the high-fidelity emulator in our RNA framework depends not only on the output of the low-fidelity emulator but also on the input variables directly, i.e., $W_{2}\left(x, W_{1}(x)\right)$. This difference in formulation is important and impacts the design of active learning strategies in our framework compared to those for linked GPs.

Similar to conventional GP emulators for single-fidelity deterministic computer models, the proposed RNA emulator also exhibits the interpolation property, which is described in the following proposition. The proof is provided in Appendix B.

Proposition 3.2. The RNA emulator satisfies interpolation property, that is, $\mu_{l}^{*}\left(\mathbf{x}_{i}^{[l]}\right)=y_{i}^{[l]}$, and $\sigma_{l}^{* 2}\left(\mathbf{x}_{i}^{[l]}\right)=0$, where $\left\{\left(\mathbf{x}_{i}^{[l]}, y_{i}^{[l]}\right)\right\}_{i=1, \ldots, n_{l}}$ are the training samples.

An example of this posterior distribution is presented in the right panel of Figure 1, illustrating that the posterior mean closely aligns with the true function, and the confidence intervals constructed by the posterior variance cover the true function. For further insights into how this nonlinear relationship modeling can effectively reconstruct the high-fidelity function $f_{2}(x)$ for this example, we refer to Perdikaris et al. (2017), which discovers a smooth mapping from a manifold to the high-fidelity data.

We have developed an $\mathrm{R}$ package called RNAmf, which implements the parameter estimation and computations for the closed-form posterior mean and variance using a Gaussian kernel and a Matérn kernel with smoothness parameters of 1.5 and 2.5.

### 3.1 Insights into the RNA emulator

We delve into the RNA emulator, exploring its mathematical insights and investigating scenarios where this method may succeed or encounter challenges. This investigation advances our understanding of the mathematics behind this emulator.

For the sake of simplicity in explanation, we consider two fidelity levels $(L=2)$ and assume the input is one-dimensional, that is, $x \in \Omega \subseteq \mathbb{R}$. According to Proposition 3.1. under a squared exponential kernel function, the RNA emulator yields the following posterior
mean:

$$
\mu_{2}^{*}(x)=\alpha_{2}+\sqrt{\frac{\theta_{2 y}}{\theta_{2 y}+2 \sigma_{1}^{* 2}(x)}} \sum_{i=1}^{n_{2}} r_{i}^{[2]} \exp \left(-\frac{\left(x-x_{i}^{[2]}\right)^{2}}{\theta_{2}}-\frac{\left(y_{i}^{[1]}-\mu_{1}^{*}(x)\right)^{2}}{\theta_{2 y}+2 \sigma_{1}^{* 2}(x)}\right)
$$

where $r_{i}^{[2]}=\left(\mathbf{K}_{2}^{-1}\left(\mathbf{y}_{2}-\alpha_{2} \mathbf{1}_{n_{2}}\right)\right)_{i}$.

The mathematical expression $\mu_{2}^{*}(x)$ reveals several insights into the behavior of the RNA emulator. Firstly, it reveals the impact of the uncertainty in the low-fidelity model, $\sigma_{1}^{* 2}(x)$, on the posterior mean $\mu_{2}^{*}(x)$. In scenarios where $\sigma_{1}^{* 2}(x)=0$ for all $x \in \Omega, \mu_{2}^{*}(x)$ mirrors the posterior mean when the low-fidelity GP prediction $\mu_{1}^{*}(x)$ is replaced with the true low-fidelity function $f_{1}(x)$. Consequently, the term $\sqrt{\frac{\theta_{2 y}}{\theta_{2 y}+2 \sigma_{1}^{* 2}(x)}}$ acts as a scaling factor for the posterior mean, adjusting the influence of the uncertainty $\sigma_{1}^{* 2}(x)$ on the overall prediction to account for the approximation error between $\mu_{1}^{*}(x)$ and $f_{1}(x)$. Additionally, the inflated denominator of $\frac{\left(y_{i}^{[1]}-\mu_{1}^{*}(x)\right)^{2}}{\theta_{2 y}+2 \sigma_{1}^{2}(x)}$ by the low-fidelity model uncertainty also aids in mitigating the approximation error, indicating a slower decay in correlation with the squared distance between the low-fidelity observations $y_{i}^{[1]}$ and the predictions from the low-fidelity emulator $\mu_{1}^{*}(x)$. Both aspects ensure a balanced integration of high and low-fidelity information, which is particularly crucial when dealing with limited samples from low-fidelity data.

Figure 3 demonstrates an example of how the low-fidelity emulator impacts RNA emulation performance. The left panel illustrates that with limited low-fidelity data $\left(n_{1}=8\right)$, especially in the absence of data at $x \in(0.3,0.8)$, the posterior mean of the low-fidelity emulator, $\mu_{1}^{*}(x)$ (represented by the green line), inaccurately predicts the true low-fidelity simulator $f_{1}(x)$ (red dashed line). In this scenario, the scaling factor (orange line), $\sqrt{\frac{\theta_{2 y}}{\theta_{2 y}+2 \sigma^{2} 1(x)}}$, where the posterior variance $\sigma_{1}^{* 2}(x)$ plays a crucial role, is very small for those poor predictions of $\mu_{1}^{*}(x)$, particularly for $x \in(0.3,0.8)$ where the factor is close to zero. This results in $\mu_{2}^{*}(x)$ being close to the mean estimate $\hat{\alpha}_{2}$. This is not surprising because there is no data available from both low-fidelity and high-fidelity simulators in this region, leading to the posterior mean reverting back to the mean estimate. With an increase in low-fidelity data $\left(n_{1}=12\right)$, which makes $\mu_{1}^{*}(x)$ much closer to the true $f_{1}(x)$, the scaling factor is close to one everywhere, significantly enhancing the accuracy of the RNA emulator.



Figure 3: Illustration of RNA emulator insights using the Perdikaris example. The left panel and right panel depict results obtained with different sample sizes of low-fidelity data (red dots), $n_{1}=8$ (left) and $n_{1}=12$ (right), alongside the same high-fidelity data (black triangles) of size $n_{2}=6$. The scaling factor is the orange solid line, with values shifted by subtracting 3.

The posterior variance can be written as (see Supplementary Materials S1)

$$
\sigma_{2}^{* 2}(x)=\mathbb{V}\left[\mathbb{E}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right]+\mathbb{E}\left[\mathbb{V}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right]
$$

where

$$
\begin{aligned}
& \mathbb{V}\left[\mathbb{E}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right] \\
= & \sum_{i, k=1}^{n_{2}} r_{i}^{[2]} r_{k}^{[2]} \exp \left(-\frac{\left(x-x_{i}^{[2]}\right)^{2}+\left(x-x_{k}^{[2]}\right)^{2}}{\theta_{2}}\right)\left(\frac{\exp \left(-a_{i k}(x)\right)}{\sqrt{1+4 \frac{\sigma_{1}^{* 2}(x)}{\theta_{2 y}}}}-\frac{\exp \left(-b_{i k}(x)\right)}{1+2 \frac{\sigma_{1}^{* 2}(x)}{\theta_{2 y}}}\right)
\end{aligned}
$$

and

$$
\begin{aligned}
& \mathbb{E}\left[\mathbb{V}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right] \\
= & \tau_{2}^{2}\left(1-\frac{1}{\sqrt{1+4 \frac{\sigma_{1}^{* 2}(\mathbf{x})}{\theta_{2 y}}}} \sum_{i, k=1}^{n_{2}}\left(\mathbf{K}_{2}^{-1}\right)_{i k} \exp \left(-\frac{\left(x-x_{i}^{[2]}\right)^{2}+\left(x-x_{k}^{[2]}\right)^{2}}{\theta_{2}}-a_{i k}(x)\right)\right)
\end{aligned}
$$



Figure 4: Illustration of decomposition of $\sigma_{2}^{* 2}(x)$ (black solid line) for the examples of Figure 3. where $V_{1}$ is the blue dashed line and $V_{2}(x)$ is the green dashed line.

where

$a_{i k}(x)=\frac{\left(\frac{y_{i}^{[1]}+y_{k}^{[1]}}{2}-\mu_{1}^{*}(x)\right)^{2}}{\frac{\theta_{2 y}}{2}+2 \sigma_{1}^{* 2}(x)}+\frac{\left(y_{i}^{[1]}-y_{k}^{[1]}\right)^{2}}{2 \theta_{2 y}} \quad$ and $\quad b_{i k}(x)=\frac{\left(y_{i}^{[1]}-\mu_{1}^{*}(x)\right)^{2}+\left(y_{k}^{[1]}-\mu_{1}^{*}(x)\right)^{2}}{\theta_{2 y}+2 \sigma_{1}^{* 2}(x)}$.

Define $V_{1}(x)=\mathbb{V}\left[\mathbb{E}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right]$ and $V_{2}(x)=\mathbb{E}\left[\mathbb{V}\left[f_{2}(x) \mid f_{1}(x), \mathbf{y}_{1}, \mathbf{y}_{2}\right]\right]$, then $V_{1}(x)$ represents the overall contribution of the GP emulator $W_{1}$ to $\sigma_{2}^{* 2}(x)$ and $V_{2}(x)$ represents the contribution of the GP emulator $W_{2}$ to $\sigma_{2}^{* 2}(x)$. This decomposition mirrors that of Ming and Guillas (2021) within the context of linked GPs. Figure 4 illustrates this decomposition for the examples in Figure 3. It can be seen that for both scenarios, $V_{2}$ appears to dominate $V_{1}$, indicating that $W_{2}$ contributes more uncertainty than $W_{1}$. However, when we have limited low-fidelity data ( $n_{1}=8$, left panel), $V_{1}$ exhibits a very high peak at $x \approx 0.04$ with a value close to 0.10 , even very close to the maximum value of $V_{2}$. From an active learning perspective, if the cost of evaluating $f_{1}(x)$ is cheaper than $f_{2}(x)$, then it's sensible to select the next sample from the cheaper $f_{1}(x)$ to reduce $\sigma_{2}^{* 2}(x)$. On the other hand, when we have more low-fidelity data ( $n_{1}=12$, right panel), $V_{1}$ remains very small everywhere compared to $V_{2}$, indicating that selecting the next sample from $f_{2}(x)$ would be more effective in reducing the predictive uncertainty. More details of active learning strategies will be introduced in the next section.

## 4 Active learning for RNA emulator

In this section, we present four active learning (AL) strategies aimed at enhancing the predictive capabilities of the proposed model through the careful design of computer experiments. Different from traditional AL approaches, these strategies encompass the dual task of not only identifying the optimal input locations but also determining the most appropriate fidelity level. These strategies are inspired by the decomposition of uncertainties discussed in Section 3.1 as well as popular sampling criteria in AL for GPs: "Active Learning MacKay (ALM)" and "Active Learning Cohn (ALC)" (Seo et al., 2000).

We suppose that an initial experiment of sample size $n_{l}$ for each fidelity level $l$, following a nested design $\mathcal{X}_{L} \subseteq \cdots \subseteq \mathcal{X}_{1}$, is conducted, for which a space-filling design is often considered, such as the nested Latin hypercube design (Qian, 2009). AL seeks to optimize

a selection criterion for choosing the next point $\mathbf{x}_{n_{l}+1}^{[l]}$ at fidelity level $l$, carrying out its corresponding simulation $y_{n_{l}+1}^{[l]}=f_{l}\left(\mathbf{x}_{n_{l}+1}^{[l]}\right)$, and thus augmenting the dataset.

### 4.1 Active Learning Decomposition (ALD)

We first introduce an active learning criterion inspired by Section 3.1 and the variance-based adaptive design for linked GPs outlined in Ming and Guillas (2021). Specifically, we extend the decomposition of (14) to encompass $L$ fidelity levels:

$$
\sigma_{L}^{* 2}(\mathbf{x})=\sum_{l=1}^{L} V_{l}(\mathbf{x})
$$

where $V_{l}(\mathbf{x})$ represents the contribution of each GP emulator $W_{l}$ at fidelity level $l$ to $\sigma_{L}^{* 2}(\mathbf{x})$ :

$$
V_{l}(\mathbf{x})=\mathbb{E} \cdots \mathbb{E} \mathbb{V} \mathbb{E} \cdots \mathbb{E}\left[f_{L}(\mathbf{x}) \mid f_{L-1}(\mathbf{x}), \cdots, f_{1}(\mathbf{x}), \mathbf{y}_{L}, \cdots, \mathbf{y}_{1}\right]
$$

with $\mathbb{V}$ being at the $l$-th term. Considering the simulation $\operatorname{cost} C_{l}$, our approach guides the selection of the next point $\mathbf{x}_{n_{l}+1}^{[l]}$ at fidelity level $l$ by maximizing the criterion, which we
refer to as Active Learning Decomposition (ALD):

$$
\left(l^{*}, \mathbf{x}_{n_{l^{*}+1}^{\left[l^{*}\right]}}\right)=\underset{l \in\{1, \ldots, L\} ; \mathbf{x} \in \Omega}{\operatorname{argmax}} \frac{V_{l}(\mathbf{x})}{\sum_{j=1}^{l} C_{j}}
$$

which aims to maximize the ratio between each contribution $V_{l}(\mathbf{x})$ to $\sigma_{L}^{* 2}(\mathbf{x})$ and the simulation cost $\sum_{j=1}^{l} C_{j}$ at each fidelity level $l$.

The reason behind assigning a simulation cost of $\sum_{j=1}^{l} C_{j}$ to the selection of $\mathbf{x}_{n_{l}+1}^{[l]}$ is because of the nested structure assumption as in (1) for fitting the RNA emulator, implying that we also need to conduct simulations with the same optimal input location at lower fidelity levels. That is, to run the simulation $f_{l^{*}}\left(\mathbf{x}_{n_{l^{*}+1}^{\left[\ell^{*}\right.}}\right)$, we also need to $\operatorname{run} f_{l}\left(\mathbf{x}_{n_{l}+1}^{[l]}\right)$ with $\mathbf{x}_{n_{l}+1}^{[l]}=\mathbf{x}_{n^{*}+1}^{\left[{ }^{*}\right]}$ for all $1 \leq l<l^{*}$. It is also worth mentioning that the cost can be tailored to depend on the input $\mathbf{x}$, as done in He et al. (2017) and Stroh et al. (2022).

### 4.2 Active Learning MacKay (ALM)

A straightforward but commonly used sampling criterion in AL is to select the next point that maximizes the posterior predictive variance (MacKay, 1992). Extending this concept to our scenario by accounting for the simulation $\operatorname{cost} C_{l}$, we choose the next point $\mathbf{x}_{n_{l}+1}^{[l]}$ at fidelity level $l$ by maximizing the ALM criterion:

$$
\left(l^{*}, \mathbf{x}_{n_{l^{*}+1}^{[*]}}^{\left[{ }^{*}\right]}\right)=\underset{l \in\{1, \ldots, L\} ; \mathbf{x} \in \Omega}{\operatorname{argmax}} \frac{\sigma_{l}^{* 2}(\mathbf{x})}{\sum_{j=1}^{l} C_{j}}
$$

Note that after running the simulation at the optimal input location $\mathbf{x}_{n^{*}+1}^{\left[l^{*}\right]}$ at level $l^{*}$, the posterior predictive variance $\sigma_{l}^{* 2}\left(\mathbf{x}_{n_{l}^{*}+1}^{\left[l^{*}\right]}\right)$ becomes zero (see Proposition 3.2). In other words, our selection of the optimal level hinges on achieving the highest ratio of uncertainty reduction at $\mathbf{x}_{n_{l^{*}+1}}^{\left[l^{*}\right]}$ to the simulation cost.

The computation of ALD and ALM criteria is facilitated by the availability of the closedform expression of the posterior predictive variance as in (12), which in turn simplifies the optimization process of (15) and (16). In particular, the optimal input location $\mathbf{x}_{n_{l}+1}^{[l]}$ for each $l$ can be efficiently obtained through the optim library in R, using the method=L-BFGS-B
option, which performs a quasi-Newton optimization approach of Byrd et al. (1995).

### 4.3 Active Learning Cohn (ALC)

Another widely employed, more aggregate criterion is Active Learning Cohn (ALC) (Cohn, 1993; Seo et al., 2000). In contrast to ALM, ALC selects an input location that maximizes the reduction in posterior variances across the entire input space after running this selected simulation. Extending the concept to our scenario by accounting for the simulation $\operatorname{cost} C_{l}$, we choose the next point $\mathbf{x}_{n_{l}+1}$ at fidelity level $l$ by maximizing the ALC criterion:



where $\Delta \sigma_{L}^{2}(l, \mathbf{x})$ is the average reduction in variance (of the highest-fidelity emulator) from the current design measured through a choice of the fidelity level $l$ and the input location $\mathbf{x}$, augmenting the design. That is,

$$
\Delta \sigma_{L}^{2}(l, \mathbf{x})=\int_{\Omega} \sigma_{L}^{* 2}(\boldsymbol{\xi})-\tilde{\sigma}_{L}^{* 2}(\boldsymbol{\xi} ; l, \mathbf{x}) \mathrm{d} \boldsymbol{\xi}
$$

where $\sigma_{L}^{* 2}(\boldsymbol{\xi})$ is the posterior variance of $f_{L}(\boldsymbol{\xi})$ based on the current design $\left\{\mathcal{X}_{l}\right\}_{l=1}^{L}$, and $\tilde{\sigma}_{L}^{* 2}(\boldsymbol{\xi} ; l, \mathbf{x})$ is the posterior variance based on the augmented design combining the current design and a new input location $\mathbf{x}$ at each fidelity level lower than or equal to $l$, i.e.,

$\left\{\left(\mathcal{X}_{1} \cup \mathbf{x}_{n_{1}+1}^{[1]}\right), \ldots,\left(\mathcal{X}_{l} \cup \mathbf{x}_{n_{l}+1}^{[l]}\right), \mathcal{X}_{l+1}, \ldots, \mathcal{X}_{L}\right\}$ with $\mathbf{x}_{n_{1}+1}^{[1]}=\cdots=\mathbf{x}_{n_{l}+1}^{[l]}=\mathbf{x}$. Once again, the incorporation of the new input location $\mathbf{x}$ at each fidelity level lower than $l$ is due to the nested structure assumption. In other words, our selection of the optimal level involves maximizing the ratio of average reduction in the variance of the highest-fidelity emulator to the associated simulation cost. In practice, the integration in (18) can be approximated by numerical methods, such as MC integration.

Unlike ALM where the influence of design augmentation on the variance of the highestfidelity emulator is unclear, ALC is specifically designed to maximize the reduction in variance of the highest-fidelity emulator. However, the ALC strategy involves requiring knowledge of future outputs $y_{n_{s}+1}^{[s]}=f_{s}\left(\mathbf{x}_{n_{s}+1}^{[s]}\right)$ for all $1 \leq s \leq l$, as they are involved
in $\tilde{\sigma}_{L}^{* 2}(\boldsymbol{\xi} ; l, \mathbf{x})$ (as seen in (13)). These outputs are not available prior to conducting the simulations, rendering the posterior variance inaccessible. A possible approach to address this issue is through MC approximation to impute the outputs. Specifically, we can impute $y_{n_{s}+1}^{[s]}$ for each $1 \leq s \leq l$ by drawing samples from the posterior distribution of $f_{s}\left(\mathbf{x}_{n_{s}+1}^{[s]}\right)$ based on the current design, which is a normal distribution with the posterior mean and variance presented in Proposition 3.1. This allows us to repeatedly compute $\tilde{\sigma}_{L}^{* 2}(\boldsymbol{\xi} ; l, \mathbf{x})$ using the imputations and average the results to approximate the variance. Notably, with the imputed output $y_{n_{s}+1}^{[s]}$, the variance $\tilde{\sigma}_{L}^{* 2}(\boldsymbol{\xi} ; l, \mathbf{x})$ can be efficiently computed using the Sherman-Morrison formula (Harville, 1998) for updating the covariance matrix's inverse, $\mathbf{K}_{l}^{-1}$, from $\sigma_{L}^{* 2}(\boldsymbol{\xi})$. The details regarding this efficient inverse covariance matrix update can be found in Chapter 6.3 of Gramacy (2020).

In contrast to ALM, searching for the optimal $l$ and $\mathbf{x}$ that maximize the ALC criterion (17) can be quite computationally expensive due to the costly MC approximation to compute (18). To this end, an alternative strategy is proposed to strike a compromise by combining the two criteria.

### 4.4 Two-step approach: ALMC

Given the distinct advantages and limitations of both ALM and ALC criteria (details of which are referred to Chapter 6 of Gramacy (2020)), for a comprehensive exploration, we can contemplate their combination. Inspired by Le Gratiet and Cannamela (2015), we introduce a hybrid approach, which we refer to as $A L M C$. First, the optimal input location is selected by maximizing the posterior predictive variance of the highest fidelity emulator:

$$
\mathbf{x}^{*}=\underset{\mathbf{x} \in \Omega}{\operatorname{argmax}} \sigma_{L}^{* 2}(\mathbf{x})
$$

Then, the ALC criterion determines the fidelity level with the identified input location:

$$
l^{*}=\underset{l \in\{1, \ldots, L\}}{\operatorname{argmax}} \frac{\Delta \sigma_{L}^{2}\left(l, \mathbf{x}^{*}\right)}{\sum_{j=1}^{l} C_{j}}
$$

Unlike ALM, this hybrid approach focuses on the direct impact on the highest-fidelity emulator. It first identifies the sampling location that maximizes $\sigma_{L}^{* 2}(\mathbf{x})$, and then determines which level selection will effectively reduce the overall variance of the highest-fidelity emulator across the input space after running this location. This synergistic approach is not only expected to capture the advantages of both ALM and ALC, but also offers computational efficiency advantages compared to the ALC method in the previous subsection. This is due to the fact that the optimization for $\mathrm{x}^{*}$ by maximizing the closed-form posterior variance is computationally much cheaper, as discussed in Section 4.2 .

Figure 5 demonstrates the effectiveness of these four strategies, showing that how they are employed to determine the next input location with a suitable fidelity level to run the simulation and augment the dataset, particularly for the example in the right panel of Figure 1 where the RNA emulator is employed. Consider the simulation costs: $C_{1}=1$ and $C_{2}=3$ for the two simulators. It shows that, for all four criteria, the choice is consistently in favor of selecting the low-fidelity simulator to augment the dataset. While the selected locations differ, ALD, ALC, and ALMC all fall within the range of $[0.18,0.25]$, which, as per the current design (prior to running this simulation), holds large uncertainty, as seen in the right panel of Figure 1. ALM selects the sample at the boundary of the input space. All these selection outcomes contribute to an overall improvement in emulation accuracy, while simultaneously reducing global uncertainty, even when opting for low-fidelity data alone, highlighting the effectiveness of the four strategies.

### 4.5 Remark on the AL strategies

In this section, we delve deeper into the merits of the AL strategies, with a focus on the conditions favoring each method. To gain deeper insights, we consider a synthetic example generated from a 2-level Currin function (Xiong et al., 2013; Kerleguer et al., 2024), with the explicit form provided in Appendix C. Assuming simulation costs $C_{1}=1$ and $C_{2}=3$, we employ the four AL strategies until reaching a total budget of 15 .

Figure 6 showcases the selected sites within the input space $[0,1]^{2}$. Similar to discussions on AL for single-fidelity GPs (Seo et al., 2000; Gramacy and Lee, 2009; Bilionis and



Figure 5: Demonstration of the four active learning strategies for selecting the next data point to augment the dataset, using the example in the right panel of Figure 1. The criteria of the four strategies are presented in the bottom panel, where the dots represent the optimal input locations for each of the simulators. Notably, ALD utilizes the gray line to illustrate $\sigma_{2}^{* 2}(x)$, which is decomposed into $V_{1}(x)$ (depicted in red) and $V_{2}(x)$ (depicted in black). ALMC, on the other hand, employs the gray line to determine the optimal input location and then utilizes the red and black lines (which are identical to $A L C$ ) to decide the fidelity level. The upper panels show the corresponding fits after adding the selected points to the training dataset, where the solid dots represent the chosen samples, all of which select the low-fidelity simulator.

Zabaras, 2012, Beck and Guillas, 2016), ALM tends to push selected data points towards the boundaries of the input space, whereas ALC avoids boundary locations. ALD and ALMC, inheriting attributes of ALM, exhibit similar behavior to ALM. The choice between them depends on the underlying true function: if the function in the boundary region is flat and exhibits more variability in the interior, then ALC may be preferable. Regarding computational efficiency, ALD, ALM, and ALMC benefit from closed-form expressions of the posterior variance, while ALC is more computationally demanding due to extensive MC sampling efforts.

It is worth noting that if the scale of low-fidelity outputs significantly exceeds that of high-fidelity outputs, ALM may consistently favor low-fidelity levels in the initial acquisitions,


Figure 6: Predictive standard deviation after acquisitions by four proposed strategies with the additional budget of 15, and the simulation costs $C_{1}=1$ and $C_{2}=3$ for Currin function.

as the maximum of the low-fidelity posterior variance tends to be large. However, it's unclear whether this selection is effective, as maximizing the posterior variance of the low-fidelity emulator doesn't necessarily translate to a reduction in the uncertainty of the high-fidelity emulator. In contrast, the other three methods focus on directly impacting the high-fidelity emulator by selecting points, making them independent of the scale. In summary, considering the discussions above and the findings from our empirical studies in Sections 55 and 6. ALD and ALMC generally emerge as favorable choices, offering accurate RNA emulators along with computational efficiency.

## 5 Numerical Studies

In this section, we conduct a suite of numerical experiments to examine the performance of the proposed approach. The experiments encompass two main aspects. In Section 5.1, we assess the predictive capabilities of the proposed RNA emulator, while Section 5.2 delves into the evaluation of the performance of the proposed AL strategies.

We consider the anisotropic squared exponential kernel as in (7) for the proposed model, a choice that is also shared by our competing methods. All experiments are performed on a MacBook Pro laptop with $2.9 \mathrm{GHz}$ 6-Core Intel Core i9 and 16Gb of RAM.

### 5.1 Emulation performance

We begin by comparing the predictive performance of the proposed RNA emulator (labeled RNAmf) with two other methods in the numerical experiments: the co-kriging model (labeled CoKriging) by Le Gratiet and Garnier (2014), and the nonlinear auto-regressive multifidelity GP (labeled NARGP) by Perdikaris et al. (2017). The two methods are readily available through open repositories, specifically the R package MuFiCokriging (Le Gratiet, 2012) and the Python package on the GitHub repository (Perdikaris, 2016), respectively.

Five synthetic examples commonly used in the literature to evaluate emulation performance in multi-fidelity simulations are considered, including the two-level Perdikaris function (Perdikaris et al., 2017; Kerleguer et al., 2024),



the two-level Park function (Park, 1991; Xiong et al., 2013),

$$
\left\{\begin{array}{l}
f_{1}(\mathbf{x})=f_{2}(\mathbf{x})+\frac{\sin \left(x_{1}\right)}{10} f_{2}(\mathbf{x})-2 x_{1}+x_{2}^{2}+x_{3}^{2}+0.5 \\
f_{2}(\mathbf{x})=\frac{x_{1}}{2}\left[\sqrt{1+\left(x_{2}+x_{3}^{2}\right) \frac{x_{4}}{x_{1}^{2}}}-1\right]+\left(x_{1}+3 x_{4}\right) \exp \left(1+\sin \left(x_{3}\right)\right)
\end{array} \text { for } \mathbf{x} \in[0,1]^{4}\right.
$$

and the three-level Branin function (Sobester et al., 2008), the two-level Borehole function (Morris et al., 1993; Xiong et al., 2013), and the two-level Currin function (Xiong et al., 2013; Kerleguer et al., 2024) given in Appendix C. Additionally, we introduce a three-level function modified from the Franke function (Franke, 1979), which is also available in Appendix C.

The data are generated by evaluating these functions at input locations obtained from the nested space-filling design introduced by Le Gratiet and Garnier (2014) with sample sizes $\left\{n_{l}\right\}_{l=1}^{L}$. The sample sizes and input dimension for each example are outlined in Table 1. To examine the prediction performance, $n_{\text {test }}=1000$ random test input locations are generated from the same input space. We evaluate the prediction performance based on two criteria: the root-mean-square error (RMSE) and continuous rank probability score

|  | Perdikaris | Branin | Park | Borehole | Currin | Franke |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $d$ | 1 | 2 | 4 | 8 | 2 | 2 |
| $n_{1}$ | 13 | 20 | 40 | 60 | 20 | 20 |
| $n_{2}$ | 8 | 15 | 20 | 30 | 10 | 15 |
| $n_{3}$ |  | 10 |  |  |  | 10 |

Table 1: Dimension and sample sizes for each of the synthetic examples.

(CRPS) Gneiting and Raftery, 2007), which are defined as follows:

$$
\begin{aligned}
\mathrm{RMSE} & =\left(\sum_{i=1}^{n_{\text {test }}} \frac{\left(f_{L}\left(\mathbf{x}_{i}^{\text {test }}\right)-\mu_{L}^{*}\left(\mathbf{x}_{i}^{\text {test }}\right)\right)^{2}}{n_{\text {test }}}\right)^{1 / 2}, \quad \text { and } \\
\mathrm{CRPS} & =\frac{1}{n_{\text {test }}} \sum_{i=1}^{n_{\text {test }}} \sigma_{L}^{*}\left(\mathbf{x}_{i}^{\text {test }}\right)\left[\frac{1}{\sqrt{\pi}}-2 \psi\left(z\left(\mathbf{x}_{i}^{\text {test }}\right)\right)-z\left(\mathbf{x}_{i}^{\text {test }}\right)\left(2 \Psi\left(z\left(\mathbf{x}_{i}^{\text {test }}\right)\right)-1\right)\right]
\end{aligned}
$$

with $z\left(\mathbf{x}_{i}^{\text {test }}\right)=\left(f_{L}\left(\mathbf{x}_{i}^{\text {test }}\right)-\mu_{L}^{*}\left(\mathbf{x}_{i}^{\text {test }}\right)\right) / \sigma_{L}^{*}\left(\mathbf{x}_{i}^{\text {test }}\right)$, where $\mathbf{x}_{i}^{\text {test }}$ is the $i$-th test input location, $\psi$ and $\Psi$ denote the probability density function and cumulative distribution function of a standard normal distribution, respectively. Note that CRPS serves as a performance metric for the posterior predictive distribution of a scalar observation. Lower values for the RMSE and CRPS indicate better model accuracy. These two metrics have been extensively used in surrogate modeling literature. See, for example, Gramacy and Lee (2008), Gramacy (2020), and Sauer et al. (2023). Additionally, we assess the computational efficiency by comparing the computation time needed for model fitting and predictions.

Figures 7 and 8 show the results of RMSE and CRPS metrics across 100 repetitions, each employing a different random nested design for the training input locations. The proposed RNAmf consistently outperforms CoKriging by both metrics, particularly for examples exhibiting nonlinear relationships between simulators, such as the Perdikaris, Borehole, Currin, and Franke functions. For instances where simulators follow a linear (or nearly linear) auto-regressive model, like the Brainin and Park functions, the proposed RNAmf remains competitive with CoKriging, which is designed to excel in such scenarios. This highlights the flexibility of our approach, enabled by the GP prior for modeling relationships. On the other hand, NARGP, another approach modeling nonlinear relationships, outperforms CoKriging in most of the examples and is competitive with RNAmf, except in the Perdikaris


Figure 7: RMSEs of six synthetic examples across 100 repetitions.

and Franke examples, where RNAmf exhibits superior performance. However, it comes with significantly higher computational costs, as shown in Figure 9, due to its expensive MC approximation, being roughly fifty times slower than both RNAmf and CoKriging on average. Notably, in scenarios involving three fidelities, including the Brainin and Franke examples, the computational time for NARGP exceeds that of RNAmf by more than 150 times. This shows that NARGP can suffer from intensive computation as the number of fidelity levels increases, while our method remains competitive in this regard. In summary, the performance across these synthetic examples underscores the capability of the proposed method in providing an accurate emulator at a reasonable computational time, facilitated by the closed-form posterior mean and variance derived in Proposition 3.1.

### 5.2 Active learning performance

With the accurate RNA emulator in place, we now investigate on the performance of AL strategies for the emulator using the proposed criteria. We compare with two existing methods: CoKriging-CV, a cokriging-based sequential design utilizing cross-validation


model 白 RNAmf 白 CoKriging 白 NARGP

Figure 8: CRPSs of six synthetic examples across 100 repetitions.



Figure 9: Computational time of six synthetic functions across 100 repetitions.

techniques (Le Gratiet and Cannamela, 2015), and MR-SUR, a sequential design maximizing the rate of stepwise uncertainty reduction using the KO model (Stroh et al., 2022). As for implementing CoKriging-CV, we utilized the code provided in the Supplementary Materials of Le Gratiet and Cannamela (2015). Notably, both of these methods employed the (linear)
autoregressive model as in (2) in their implementations. To maintain a consistent comparison, we use the one-dimensional Perdikaris function (nonlinear) and the 4-dimensional Park function (linear autoregressive) in Section 5.1, to illustrate the performance of these methods.

In this experiment, we suppose that the simulation costs associated with the low- and high-fidelity simulators are $C_{1}=1$ and $C_{2}=3$, respectively. The initial data is established similar to Section 5.1, with sample sizes specified in Table 1. We consider a total simulation budget of $C_{\text {total }}=80$ for the Perdikaris function and $C_{\text {total }}=130$ for the Park function. For ALC and ALMC acquisitions which involve the computation of the average reduction in variance as in (18), 1000 and 100 uniform samples are respectively generated from the input space to approximate the integral and impute the future outputs.

Figure 10 shows the results of RMSE and CRPS metrics for the Perdikaris function, with respect to the total simulation costs accrued after each sample selection. The left panel of Figure 11 displays a boxplot depicting the final RMSEs after reaching the total simulation budget across the 10 repetitions. The results show that the proposed AL methods dramatically outperform the two competing methods, CoKriging-CV and MR-SUR, in terms of both accuracy and stability, considering the same costs. As the cost increases, MR-SUR begins to close the gap, while CoKriging-CV lags behind the other methods. Among the four proposed AL strategies, the distinctions are minimal. As noted in Section 4. ALC acquisitions involve intricate numerical integration approximations and data imputation, taking approximately 400 seconds for each acquisition in this example. In contrast, ALD, ALM and ALMC are significantly more computationally efficient due to the closed-form nature of the posterior variance, requiring only around 1, 1, and 10 seconds per acquisition, respectively.

From the right panel of Figure 11, it can be seen that the proposed AL methods tend to select low-fidelity simulators more frequently than the other two comparative methods, notably MR-SUR, which consistently chooses samples exclusively from the high-fidelity simulator. This suggests that the proposed RNA model can effectively infer the high-fidelity simulation using primarily low-fidelity data for the nonlinear Perdikaris function, while the other two KO-based methods (CoKriging-CV and MR-SUR) require more high-fidelity data



Figure 10: RMSE and CRPS for the Perdikaris function with respect to the simulation cost. Solid lines represent the average over 10 repetitions and shaded regions represent the ranges.



Figure 11: Final RMSE (left) and proportion of $A L$ acquisitions choosing low-fidelity data (right) for the Perdikaris function. Boxplots indicate spread over 10 repetitions.

to reduce the uncertainty.

Figures 12 and 13 present the results for the Park function. As expected, the distinctions between these strategies are not as significant because the function aligns more closely with the KO model (linear autoregressive). Nonetheless, our proposed AL strategies still exhibit better average performance. At the final cost budget of $C_{\text {total }}=130$, ALD and ALMC perform the best, collecting a larger portion of high-fidelity data, as indicated in

Figure 13. In contrast, the KO-based strategies collect more low-fidelity data, which is again expected because KO-based models are efficient at leveraging low-fidelity data to infer the high-fidelity simulator. In these scenarios, our strategies efficiently prioritize the selection of high-fidelity data to minimize uncertainty, resulting in superior prediction accuracy at the same cost.



Figure 12: $R M S E$ and CRPS for the Park function with respect to the simulation cost. Solid lines represent the average over 10 repetitions and shaded regions represent the ranges.


Figure 13: Final RMSE (left) and proportion of $A L$ acquisitions choosing low-fidelity data (right) for the Park function. Boxplots indicate spread over 10 repetitions.

## 6 Thermal Stress Analysis of Jet Engine Turbine Blade

We leverage our proposed method for a real application involving the analysis of thermal stress in a jet turbine engine blade under steady-state operating conditions. The turbine blade, which forms part of the jet engine, is constructed from nickel alloys capable of withstanding extremely high temperatures. It is crucial for the blade's design to ensure that it can endure stress and deformations while avoiding mechanical failure and friction between the blade tip and the turbine casing. We refer more details to Carter (2005), Wright and Han (2006), Sung et al. (2024a) and Sung et al. (2024b).

This problem can be treated as a static structural model and can be solved numerically using finite element methods. There are two input variables denoted as $x_{1}$ and $x_{2}$, which represent the pressure load on the pressure and suction sides of the blade, both of which fall within the range of 0.25 to $0.75 \mathrm{MPa}$, i.e., $\mathbf{x}=\left(x_{1}, x_{2}\right) \in \Omega=[0.25,0.75]^{2}$. The response of interest is the maximum value over the thermal stress profile, which is a critical parameter used to assess the structural stability of the turbine blade. We perform finite element simulations using the Partial Differential Equation Toolbox in MATLAB (MATLAB, 2021).

The simulations are conducted at two fidelity levels, each using different mesh densities for finite element methods. A denser mesh provides higher fidelity and more accurate results but demands greater computational resources. Conversely, a coarser mesh sacrifices some accuracy for reduced computational cost. Figure 14 visually presents the turbine blade structure and thermal stress profiles obtained at these two fidelity levels for the input location $\mathbf{x}=(0.5,0.45)$.


Figure 14: Illustration of low-fidelity (left) and high-fidelity (right) finite element simulations at the input setting $\mathbf{x}=(0.5,0.45)$ in the turbine blade application.

We perform the finite element simulations with sample sizes of $n_{1}=20$ and $n_{2}=10$ to examine the emulation performance. Similar to Section 5.1, we use the nested spacefilling designs of Le Gratiet and Garnier (2014) to generate the input locations of the two-fidelity computer experiments. We record the simulation time of the finite element simulations, which are respectively $C_{1}=2.25$ and $C_{2}=6.85$ (seconds) and will be used later for comparing AL strategies. To examine the performance, we conduct the high-fidelity simulations (i.e. $f_{2}(\mathbf{x})$ ) at the test input locations of size $n_{\text {test }}=100$ generated from a set of Latin hypercube samples from the same design space. The experiment is repeated 10 times, each time considering different nested space-filling designs for the training input locations.

Figure 15 presents a comparison of emulation performance with the other two competing methods, CoKriging and NARGP. Our proposed method, RNAmf, outperforms the other two methods in terms of RMSE and CRPS. While NARGP delivers competitive prediction performance, it comes at a significantly higher computational cost compared to RNAmf.



Figure 15: RMSE, CRPS, and computation time across 10 repetitions in the turbine blade application.

Figures 16 and 17 present a comparison of the AL strategies with a fixed cost budget of $C_{\text {total }}=160$ seconds. The right panel of Figure 17 reveals that these strategies collect a similar number of low-fidelity data points. Notably, CoKriging-CV exhibits significant variability across the 10 repetitions, so we have removed the shaded region and only show the average, indicating that it yields poorer prediction performance compared to the other strategies. Another KO-based strategy, MR-SUR, performs better but still falls short of our proposed AL strategies at any given simulation cost. Conversely, our proposed AL strategies
demonstrate effective results and outperform the others. This is evident from RMSE and CRPS values exhibiting a leveling-off trend, with final results around 10 and 5, respectively, compared to the initial designs yielding both metrics averaging around 15 and 7. Among the AL strategies, ALC appears superior to the other three at the final cost budget.




Figure 16: RMSE and CRPS for the turbine blade application with respect to the cost. Solid lines represent the average over 10 repetitions and shaded regions represent the ranges.



Figure 17: Final RMSE (left) and proportion of $A L$ acquisitions choosing low-fidelity data (right) for the turbine blade application. Boxplots indicate spread over 10 repetitions.

## 7 Conclusion

Multi-fidelity computer experiments have become an essential tool in simulating complex scientific problems. This paper introduces a new emulator, the RNA emulator, tailored for multi-fidelity simulations, which proves effective in producing accurate, efficient predictions for high-fidelity simulations, especially when dealing with nonlinear relationships between simulators. Building upon this new emulator, we present four AL strategies designed to select optimal input locations and fidelity levels to augment data, thereby enhancing emulation performance. Our numerical studies, including a jet turbine blade analysis, underscore the effectiveness of our approach in efficiently emulating multi-fidelity computer experiments and ensuring accurate emulation through our AL strategies.

With the RNA emulator's success, it is worthwhile to explore emulators and AL strategies built upon similar principles for addressing multi-fidelity problems with tunable fidelity parameters, such as mesh density (Picheny et al., 2013; Tuo et al., 2014). Designing experiments for such scenarios presents intriguing challenges, as shown in recent studies (see, e.g., Shaowu Yuchi et al. (2023) and Sung et al. (2024a)). Furthermore, considering the increasing prevalence of stochastic computer models (Baker et al., 2022), extending the proposed RNA emulator to accommodate noisy data would significantly enhance its relevance in real-world applications. While this article assumes noise-free data, introducing noise into the model is a feasible endeavor, a task we leave for our future research.

Supplemental Materials Additional supporting materials can be found in Supplemental Materials, including the closed-form posterior mean and variance under a squared exponential kernel and a Matérn kernel with the smoothness parameter $\nu=1.5$ and $\nu=2.5$. The $\mathrm{R}$ code and package for reproducing the results in Sections 5 and 6 are also provided.

## Appendices

## A Matérn kernel functions

Here are the Matérn kernels with smoothness parameters of 1.5 and 2.5, both of which are widely used and come with simpler expressions as follows:

$$
K_{l}\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=\phi\left(y, y^{\prime} ; \theta_{l y}\right) \prod_{j=1}^{d} \phi\left(x_{j}, x_{j}^{\prime} ; \theta_{l j}\right)
$$

with

$$
\phi\left(x, x^{\prime} ; \theta\right)=\left(1+\frac{\sqrt{3}\left|x-x^{\prime}\right|}{\theta}\right) \exp \left(-\frac{\sqrt{3}\left|x-x^{\prime}\right|}{\theta}\right)
$$

for smoothness parameter of 1.5 and

$$
\phi\left(x, x^{\prime} ; \theta\right)=\left(1+\frac{\sqrt{5}\left|x-x^{\prime}\right|}{\theta}+\frac{5\left(x-x^{\prime}\right)^{2}}{3 \theta^{2}}\right) \exp \left(-\frac{\sqrt{5}\left|x-x^{\prime}\right|}{\theta}\right)
$$

for smoothness parameter of 2.5 .

## B Proof of Proposition 3.2

For $l=1$, since $\mu_{1}^{*}(\mathbf{x})=\mu_{1}(\mathbf{x})$ and $\sigma_{1}^{* 2}(\mathbf{x})=\sigma_{1}^{2}(\mathbf{x})$ are the posterior mean and variance of a conventional GP, it can be shown that $\mu_{1}^{*}\left(\mathbf{x}_{i}^{[1]}\right)=y_{i}^{[1]}$ and $\sigma_{1}^{* 2}\left(\mathbf{x}_{i}^{[1]}\right)=0$ Santner et al. 2018). For $l=k-1$, suppose that $\mu_{k-1}^{*}\left(\mathbf{x}_{i}^{[k-1]}\right)=y_{i}^{[k-1]}$ and $\sigma_{k-1}^{* 2}\left(\mathbf{x}_{i}^{[k-1]}\right)=0$, implying that $f_{k-1}\left(\mathbf{x}_{i}^{[k-1]}\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k-1}$ remains constant at the value $y_{i}^{[k-1]}$. Then,

$$
\begin{aligned}
\mu_{k}^{*}\left(\mathbf{x}_{i}^{[k]}\right) & =\mathbb{E}\left[f_{k}\left(x_{i}^{[k]}\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right]=\mathbb{E}\left[\mathbb{E}\left[f_{k}\left(\mathbf{x}_{i}^{[k]}\right) \mid f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right), \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right]\right] \\
& =\mathbb{E}\left[\mu_{k}\left(\mathbf{x}_{i}^{[k]}, f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right)\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right] \\
& =\mu_{k}\left(\mathbf{x}_{i}^{[k]}, y_{i}^{[k-1]}\right)=y_{i}^{[k]}
\end{aligned}
$$

and

$$
\begin{aligned}
\sigma_{k}^{* 2}\left(\mathbf{x}_{i}^{[k]}\right) & =\mathbb{V}\left[f_{k}\left(\mathbf{x}_{i}^{[k]}\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right] \\
& =\mathbb{V}\left[\mathbb{E}\left[f_{k}\left(\mathbf{x}_{i}^{[k]}\right) \mid f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right), \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right]\right]+\mathbb{E}\left[\mathbb{V}\left[f_{k}\left(\mathbf{x}_{i}^{[k]}\right) \mid f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right), \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right]\right] \\
& =\mathbb{V}\left[\mu_{k}\left(\mathbf{x}_{i}^{[k]}, f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right)\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right]+\mathbb{E}\left[\sigma_{k}^{2}\left(\mathbf{x}_{i}^{[k]}, f_{k-1}\left(\mathbf{x}_{i}^{[k]}\right)\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k}\right] \\
& =0+\sigma_{k}^{2}\left(\mathbf{x}_{i}^{[k]}, y_{i}^{[k-1]}\right)=0
\end{aligned}
$$

where $\mu_{k}^{2}\left(\mathbf{x}, f_{k-1}(\mathbf{x})\right)$ and $\sigma_{k}^{2}\left(\mathbf{x}, f_{k-1}(\mathbf{x})\right)$ are defined in (10) and (11). In both of the derivations, the second to last equation holds because of $\mathbf{x}_{i}^{[k]}=\mathbf{x}_{i}^{[k-1]}$ and $f_{k-1}\left(\mathbf{x}_{i}^{[k-1]}\right) \mid \mathbf{y}_{1}, \ldots, \mathbf{y}_{k-1}:=$ $y_{i}^{[k]}$. The last equation holds because of the interpolation property of conventional GPs. By induction, this finishes the proof.

## C Synthetic functions in Section 5.1

## C. 1 Three-level Branin function

$$
\left\{\begin{array}{l}
f_{1}(\mathbf{x})=f_{2}(1.2(\mathbf{x}+2))-3 x_{2}+1 \\
f_{2}(\mathbf{x})=10 \sqrt{f_{3}(\mathbf{x})}+2\left(x_{1}-0.5\right)-3\left(3 x_{2}-1\right)-1 \quad \text { for } \mathbf{x} \in[-5,10] \times[0,15] \\
f_{3}(\mathbf{x})=\left(\frac{-1.27 x_{2}^{2}}{\pi^{2}}+\frac{5 x_{1}}{\pi}+x_{2}-6\right)^{2}+\left(10-\frac{5}{4 \pi}\right) \cos \left(x_{1}\right)+10
\end{array}\right.
$$

## C. 2 Two-level borehole function

$$
\left\{\begin{aligned}
f_{1}(\mathbf{x}) & =\frac{2 \pi T_{u}\left(H_{u}-H_{l}\right)}{2 L T_{u}}+\frac{T_{u}}{\ln \left(r / r_{w}\right)\left(1+\frac{2 n}{\ln \left(r / r_{w} r_{w}^{2} K_{w}\right.}+\frac{T_{1}}{T_{l}}\right)} \\
f_{2}(\mathbf{x}) & =\frac{5 T_{u}\left(H_{u}-H_{l}\right)}{\ln \left(r / r_{w}\right)\left(1.5+\frac{2 L T_{u}}{\ln \left(r / r_{w}\right) r_{w}^{2} K_{w}}+\frac{T_{u}}{T_{l}}\right)}
\end{aligned}\right.
$$

where $r_{w} \in[0.05,0.15], r \in[100,50000], T_{u} \in[63070,115600], H_{u} \in[990,1110], T_{l} \in$ [63.1,116], $H_{l} \in[700,820], L \in[1120,1680]$ and $K_{w} \in[9855,12045]$.

## C. 3 Two-level Currin function

$$
\left\{\begin{aligned}
f_{1}(\mathbf{x}) & =\frac{1}{4}\left[f_{2}\left(x_{1}+0.05, x_{2}+0.05\right)+f_{2}\left(x_{1}+0.05, \max \left(0, x_{2}-0.05\right)\right)\right] \\
& +\frac{1}{4}\left[f_{2}\left(x_{1}-0.05, x_{2}+0.05\right)+f_{2}\left(x_{1}-0.05, \max \left(0, x_{2}-0.05\right)\right)\right] \text { for } \mathbf{x} \in[0,1]^{2} \\
f_{2}(\mathbf{x}) & =\left[1-\exp \left(-\frac{1}{2 x_{2}}\right)\right] \frac{2300 x_{1}^{3}+1900 x_{1}^{2}+2092 x_{1}+60}{100 x_{1}^{3}+500 x_{1}^{2}+4 x_{1}+20}
\end{aligned}\right.
$$

## C. 4 Three-level Franke function

$$
\left\{\begin{aligned}
f_{1}(\mathbf{x}) & =0.75 \exp \left(-\frac{\left(9 x_{1}-2\right)^{2}}{4}-\frac{\left(9 x_{2}-2\right)^{2}}{4}\right)+0.75 \exp \left(-\frac{\left(9 x_{1}+1\right)^{2}}{49}-\frac{9 x_{2}+1}{10}\right) \\
\quad & 0.5 \exp \left(-\frac{\left(9 x_{1}-7\right)^{2}}{4}-\frac{\left(9 x_{2}-3\right)^{2}}{4}\right)-0.2 \exp \left(-\left(9 x_{1}-4\right)^{2}-\left(9 x_{2}-7\right)^{2}\right) \\
f_{2}(\mathbf{x}) & =\exp \left(-1.4 f_{1}(\mathbf{x})\right) \cos \left(3.5 \pi f_{1}(\mathbf{x})\right) \\
f_{3}(\mathbf{x}) & =\sin \left(2 \pi\left(f_{2}(\mathbf{x})-1\right)\right)
\end{aligned}\right.
$$

## References

Baker, E., Barbillon, P., Fadikar, A., Gramacy, R. B., Herbei, R., Higdon, D., Huang, J., Johnson, L. R., Ma, P., Mondal, A., Pires, B., Sacks, J., and Sokolov, V. (2022). Analyzing stochastic computer models: A review with opportunities. Statistical Science, 37(1):64-89.

Beck, J. and Guillas, S. (2016). Sequential design with mutual information for computer experiments (MICE): Emulation of a tsunami model. SIAM/ASA Journal on Uncertainty Quantification, 4(1):739-766.

Bilionis, I. and Zabaras, N. (2012). Multi-output local gaussian process regression: Applications to uncertainty quantification. Journal of Computational Physics, 231(17):5718-5746.

Binois, M., Huang, J., Gramacy, R. B., and Ludkovski, M. (2019). Replication or exploration? sequential design for stochastic simulation experiments. Technometrics, 61(1):7-23.

Byrd, R. H., Lu, P., Nocedal, J., and Zhu, C. (1995). A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190-1208.

Carter, T. J. (2005). Common failures in gas turbine blades. Engineering Failure Analysis, 12(2):237-247.

Cohn, D. (1993). Neural network exploration using optimal experiment design. Advances in Neural Information Processing Systems, 6:1071-1083.

Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and González, J. (2019). Deep gaussian processes for multi-fidelity modeling. arXiv preprint arXiv:1903.07320.

Demeyer, S., Fischer, N., and Marquis, D. (2017). Surrogate model based sequential sampling estimation of conformance probability for computationally expensive systems: application to fire safety science. Journal de la société française de statistique, 158(1):111-138.

Ehara, A. and Guillas, S. (2023). An adaptive strategy for sequential designs of multilevel computer experiments. International Journal for Uncertainty Quantification, 13(4):61-98.

Franke, R. (1979). A critical comparison of some methods for interpolation of scattered data. Technical report, Monterey, California: Naval Postgraduate School.

Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359-378.

Gramacy, R. B. (2020). Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences. CRC press.

Gramacy, R. B. and Lee, H. K. (2009). Adaptive design and analysis of supercomputer experiments. Technometrics, 51(2):130-145.

Gramacy, R. B. and Lee, H. K. H. (2008). Bayesian treed Gaussian process models with an application to computer modeling. Journal of the American Statistical Association, 103(483):1119-1130.

Haaland, B. and Qian, P. Z. G. (2010). An approach to constructing nested space-filling designs for multi-fidelity computer experiments. Statistica Sinica, 20(3):1063-1075.

Harville, D. A. (1998). Matrix Algebra from a Statistician's Perspective. New York: SpringerVerlag.

He, X., Tuo, R., and Wu, C. F. J. (2017). Optimization of multi-fidelity computer experiments via the EQIE criterion. Technometrics, 59(1):58-68.

Huang, D., Allen, T. T., Notz, W. I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-fidelity evaluations. Structural and Multidisciplinary Optimization, 32:369382.

Ji, Y., Mak, S., Soeder, D., Paquet, J.-F., and Bass, S. A. (2024). A graphical multi-fidelity Gaussian process model, with application to emulation of expensive computer simulations. Technometrics, to appear.

Kennedy, M. C. and O'Hagan, A. (2000). Predicting the output from a complex computer code when fast approximations are available. Biometrika, 87(1):1-13.

Kerleguer, B., Cannamela, C., and Garnier, J. (2024). A bayesian neural network approach to multi-fidelity surrogate modelling. International Journal for Uncertainty Quantification, 14(1):43-60.

Ko, J. and Kim, H. (2022). Deep gaussian process models for integrating multifidelity experiments with nonstationary relationships. IISE Transactions, 54(7):686-698.

Koermer, S., Loda, J., Noble, A., and Gramacy, R. B. (2023). Active learning for simulator calibration. arXiv preprint arXiv:2301.10228.

Kuya, Y., Takeda, K., Zhang, X., and Forrester, A. I. (2011). Multifidelity surrogate modeling of experimental and computational aerodynamic data sets. AIAA Journal, 49(2):289-298.

Kyzyurova, K. N., Berger, J. O., and Wolpert, R. L. (2018). Coupling computer models through linking their statistical emulators. SIAM/ASA Journal on Uncertainty Quantification, 6(3):1151-1171.

Le Gratiet, L. (2012). MuFiCokriging: Multi-Fidelity Cokriging models. R package version 1.2 .

Le Gratiet, L. (2013). Bayesian analysis of hierarchical multifidelity codes. SIAM/ASA Journal on Uncertainty Quantification, 1(1):244-269.

Le Gratiet, L. and Cannamela, C. (2015). Cokriging-based sequential design strategies using fast cross-validation techniques for multi-fidelity computer codes. Technometrics, 57(3):418-427.

Le Gratiet, L. and Garnier, J. (2014). Recursive co-kriging model for design of computer experiments with multiple levels of fidelity. International Journal for Uncertainty Quantification, 4(5):365-386.

Li, S., Xing, W., Kirby, R., and Zhe, S. (2020). Multi-fidelity Bayesian optimization via deep neural networks. Advances in Neural Information Processing Systems, 33:8521-8531.

Ma, P., Karagiannis, G., Konomi, B. A., Asher, T. G., Toro, G. R., and Cox, A. T. (2022). Multifidelity computer model emulation with high-dimensional output: An application to storm surge. Journal of the Royal Statistical Society Series C: Applied Statistics, 71(4):861-883.

MacKay, D. J. C. (1992). Information-based objective functions for active data selection. Neural Computation, 4(4):590-604.

Mak, S., Sung, C.-L., Wang, X., Yeh, S.-T., Chang, Y.-H., Joseph, V. R., Yang, V., and Wu, C. F. J. (2018). An efficient surrogate model for emulation and physics extraction of large eddy simulations. Journal of the American Statistical Association, 113(524):1443-1456.

MATLAB (2021). MATLAB version 9.11.0.1769968 (R2021b). The Mathworks, Inc., Natick, Massachusetts.

McKay, M. D., Beckman, R. J., and Conover, W. J. (2000). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 42(1):55-61.

Meng, X., Babaee, H., and Karniadakis, G. E. (2021). Multi-fidelity Bayesian neural networks: Algorithms and applications. Journal of Computational Physics, 438:110361.

Meng, X. and Karniadakis, G. E. (2020). A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. Journal of Computational Physics, 401:109020.

Ming, D. and Guillas, S. (2021). Linked Gaussian process emulation for systems of computer models using Matérn kernels and adaptive design. SIAM/ASA Journal on Uncertainty Quantification, 9(4):1615-1642.

Ming, D., Williamson, D., and Guillas, S. (2023). Deep Gaussian process emulation using stochastic imputation. Technometrics, 65(2):150-161.

Morris, M. D., Mitchell, T. J., and Ylvisaker, D. (1993). Bayesian design and analysis of computer experiments: use of derivatives in surface prediction. Technometrics, 35(3):243255.

Park, C., Waelder, R., Kang, B., Maruyama, B., Hong, S., and Gramacy, R. B. (2023). Active learning of piecewise Gaussian process surrogates. arXiv preprint arXiv:2301.08789.

Park, J. S. (1991). Tuning Complex Computer Codes to Data and Optimal Designs. University of Illinois at Urbana-Champaign.

Patra, A., Batra, R., Chandrasekaran, A., Kim, C., Huan, T. D., and Ramprasad, R. (2020). A multi-fidelity information-fusion approach to machine learn and predict polymer bandgap. Computational Materials Science, 172:109286.

Perdikaris, P. (2016). Multi-fidelity modeling using Gaussian processes and nonlinear auto-regressive schemes. https://github.com/paraklas/NARGP.

Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N. D., and Karniadakis, G. E. (2017). Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751.

Picheny, V., Ginsbourger, D., Richet, Y., and Caplin, G. (2013). Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2-13.

Qian, P. Z. G. (2009). Nested latin hypercube designs. Biometrika, 96(4):957-970.

Qian, P. Z. G., Ai, M., and Wu, C. F. J. (2009). Construction of nested space-filling designs. Annals of Statistics, 37(6A):3616-3643.

Qian, P. Z. G. and Wu, C. F. J. (2008). Bayesian hierarchical modeling for integrating low-accuracy and high-accuracy experiments. Technometrics, 50(2):192-204.

Qian, Z., Seepersad, C. C., Joseph, V. R., Allen, J. K., and Wu, C. F. J. (2006). Building surrogate models based on detailed and approximate simulations. Journal of Mechanical Design, 128(4):668-677.

R Core Team (2018). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.

Rasmussen, C. E. and Williams, C. K. (2006). Gaussian Processes for Machine Learning. Cambridge, MA: MIT Press.

Santner, T. J., Williams, B. J., and Notz, W. I. (2018). The Design and Analysis of Computer Experiments. Springer New York.

Sauer, A., Gramacy, R. B., and Higdon, D. (2023). Active learning for deep Gaussian process surrogates. Technometrics, 65(1):4-18.

Seo, S., Wallat, M., Graepel, T., and Obermayer, K. (2000). Gaussian process regression: Active data selection and test point rejection. In Mustererkennung 2000: 22. DAGMSymposium. Kiel, 13.-15. September 2000, pages 27-34. Springer.

Shaowu Yuchi, H., Roshan Joseph, V., and Wu, C. F. J. (2023). Design and analysis of multifidelity finite element simulations. Journal of Mechanical Design, 145(6):061703.

Sobester, A., Forrester, A., and Keane, A. (2008). Engineering Design via Surrogate Modelling: A Practical Guide. John Wiley \& Sons.

Stein, M. L. (1999). Interpolation of Spatial Data: Some Theory for Kriging. Springer Science \& Business Media.

Stroh, R., Bect, J., Demeyer, S., Fischer, N., Marquis, D., and Vazquez, E. (2022). Sequential design of multi-fidelity computer experiments: maximizing the rate of stepwise uncertainty reduction. Technometrics, 64(2):199-209.

Sung, C.-L., Hung, Y., Rittase, W., Zhu, C., and Wu, C. F. J. (2020). Calibration for computer experiments with binary responses and application to cell adhesion study. Journal of the American Statistical Association, 115(532):1664-1674.

Sung, C.-L., Ji, Y., Mak, S., Wang, W., and Tang, T. (2024a). Stacking designs: Designing multifidelity computer experiments with target predictive accuracy. SIAM/ASA Journal on Uncertainty Quantification, 12(1):157-181.

Sung, C.-L., Wang, W., Ding, L., and Wang, X. (2024b). Mesh-clustered Gaussian process emulator for partial differential equation boundary value problems. Technometrics, to appear.

Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. Advances in Neural Information Processing Systems, 26:2004-2012.

Tuo, R., Wu, C. F. J., and Yu, D. (2014). Surrogate modeling of computer experiments with different mesh densities. Technometrics, 56(3):372-380.

Wright, L. M. and Han, J.-C. (2006). Enhanced internal cooling of turbine blades and vanes. The Gas Turbine Handbook, 4:1-5.

Xiong, S., Qian, P. Z. G., and Wu, C. F. J. (2013). Sequential design and analysis of high-accuracy and low-accuracy computer codes. Technometrics, 55(1):37-46.

## Supplementary Materials for "Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments"

## S1 Proof of Proposition 3.1

For notational simplicity, we denote $\mathbf{Y}_{l}=\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{l}\right\}$. Based on the squared exponential kernel, the posterior mean and variance at the input $\mathbf{x}$ can be derived as follows,

$$
\begin{aligned}
\mu_{l}^{*}(\mathbf{x}) & =\mathbb{E}\left[f_{l}(\mathbf{x}) \mid \mathbf{Y}_{l}\right] \\
& =\mathbb{E}\left[\mathbb{E}\left[f_{l}(\mathbf{x}) \mid f_{l-1}(\mathbf{x}), \mathbf{Y}_{l}\right]\right] \\
& =\alpha_{l}+\mathbb{E}\left[\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mid \mathbf{Y}_{l}\right] \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right) \\
& =\alpha_{l}+\sum_{i=1}^{n_{l}} r_{i} \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{i j}^{[l]}\right)^{2}}{\theta_{l j}}\right) \mathbb{E}\left[\left.\exp \left\{-\frac{\left(y_{i}^{[l-1]}-f_{l-1}(\mathbf{x})\right)^{2}}{\theta_{l y}}\right\} \right\rvert\, \mathbf{Y}_{l}\right] \\
& =\alpha_{l}+\sum_{i=1}^{n_{l}} r_{i} \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{i j}^{[l]}\right)^{2}}{\theta_{l j}}\right) \frac{1}{\sqrt{1+2 \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{\theta_{l y}+2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)
\end{aligned}
$$

and

$$
\begin{aligned}
\sigma_{l}^{* 2}(\mathbf{x})= & \mathbb{V}\left[f_{l}(\mathbf{x}) \mid \mathbf{Y}_{l}\right] \\
= & \mathbb{V}\left[\mathbb{E}\left[f_{l}(\mathbf{x}) \mid f_{l-1}(\mathbf{x}), \mathbf{Y}_{l}\right]\right]+\mathbb{E}\left[\mathbb{V}\left[f_{l}(\mathbf{x}) \mid f_{l-1}(\mathbf{x}), \mathbf{Y}_{l}\right]\right] \\
= & \mathbb{E}\left[\left\{\mathbb{E}\left[f_{l}(\mathbf{x}) \mid f_{l-1}(\mathbf{x}), \mathbf{Y}_{l}\right]\right\}^{2}\right]-\mu_{l}^{*}(\mathbf{x})^{2}+\mathbb{E}\left[\mathbb{V}\left[f_{l}(\mathbf{x}) \mid f_{l-1}(\mathbf{x}), \mathbf{Y}_{l}\right]\right] \\
= & \mathbb{E}\left[\left\{\alpha_{l}+\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right\}^{2} \mid \mathbf{Y}_{l}\right]-\mu_{l}^{*}(\mathbf{x})^{2} \\
& \quad+\mathbb{E}\left[\tau_{l}^{2}\left\{1-\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1} \mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right\} \mid \mathbf{Y}_{l}\right] \\
= & \alpha_{l}^{2}+2 \alpha_{l}\left(\mu_{l}^{*}(\mathbf{x})-\alpha_{l}\right)+\mathbb{E}\left[\left\{\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right\}^{2} \mid \mathbf{Y}_{l}\right] \\
& \quad-\mu_{l}^{*}(\mathbf{x})^{2}+\tau_{l}^{2}-\tau_{l}^{2} \mathbb{E}\left[\left\{\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1} \mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right\} \mid \mathbf{Y}_{l}\right] \\
= & \tau_{l}^{2}-\left(\mu_{l}^{*}(\mathbf{x})-\alpha_{l}\right)^{2}+\left(\sum_{i, k=1}^{n_{l}} \zeta_{i k}\left(r_{i} r_{k}-\tau_{l}^{2}\left(\mathbf{K}_{l}^{-1}\right)_{i k}\right) \prod_{j=1}^{d} \exp \left(-\frac{\left(x_{j}-x_{i j}^{[l]}\right)^{2}+\left(x_{j}-x_{k j}^{[l]}\right)^{2}}{\theta_{l j}}\right)\right)
\end{aligned}
$$

where

$$
\begin{aligned}
& r_{i}=\left(\mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right)_{i} \\
& \zeta_{i k}=\frac{1}{\sqrt{1+4 \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}} \exp \left(-\frac{\left.\frac{y_{i}^{[l-1]}+y_{k}^{[l-1]}}{2}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{\frac{\theta_{l y}}{2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})}-\frac{\left(y_{i}^{[l-1]}-y_{k}^{[l-1]}\right)^{2}}{2 \theta_{l y}}\right)
\end{aligned}
$$

## S2 Posterior mean and variance under Matérn kernel

The posterior mean and variance at the input $\mathbf{x}$ can be derived as follows,

$$
\begin{aligned}
& \mu_{l}^{*}(\mathbf{x})= \mathbb{E}\left[f_{l}(\mathbf{x}) \mid \mathbf{Y}_{l}\right] \\
&= \alpha_{l}+\mathbb{E}\left[\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mid \mathbf{Y}_{l}\right] \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right) \\
&= \alpha_{l}+\sum_{i=1}^{n_{l}} r_{i} \xi_{i} \prod_{j=1}^{d} \phi\left(x_{j}, x_{i j}^{[l]} ; \theta_{l j}\right) \\
& \sigma_{l}^{* 2}(\mathbf{x})=\mathbb{V}\left[f_{l}(\mathbf{x}) \mid \mathbf{Y}_{l}\right] \\
&= \alpha_{l}^{2}+2 \alpha_{l}\left(\mu_{l}^{*}(\mathbf{x})-\alpha_{l}\right)+\mathbb{E}\left[\left\{\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right)\right\}^{2} \mid \mathbf{Y}_{l}\right] \\
& \quad-\mu_{l}^{*}(\mathbf{x})^{2}+\tau_{l}^{2}-\tau_{l}^{2} \mathbb{E}\left[\left\{\mathbf{k}_{l}^{T}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right) \mathbf{K}_{l}^{-1} \mathbf{k}_{l}\left(\mathbf{x}, f_{l-1}(\mathbf{x})\right)\right\} \mid \mathbf{Y}_{l}\right] \\
&= \tau_{l}^{2}-\left(\mu_{l}^{*}(\mathbf{x})-\alpha_{l}\right)^{2}+\left(\sum_{i, k=1}^{n_{l}} \zeta_{i k}\left(r_{i} r_{k}-\tau_{l}^{2}\left(\mathbf{K}_{l}^{-1}\right)_{i k}\right) \prod_{j=1}^{d} \phi\left(x_{j}, x_{i j}^{[l]} ; \theta_{l j}\right) \phi\left(x_{j}, x_{k j}^{[l]} ; \theta_{l j}\right)\right)
\end{aligned}
$$

where $\phi$ is defined in Appendix A, $\left(r_{1}, \ldots, r_{n_{l}}\right)=\mathbf{K}_{l}^{-1}\left(\mathbf{y}_{l}-\alpha_{l} \mathbf{1}_{n_{l}}\right), \xi_{i}=\mathbb{E}\left[\phi\left(f_{l-1}(\mathbf{x}), y_{i}^{[l-1]} ; \theta_{l y}\right) \mid \mathbf{Y}_{l}\right]$, and $\zeta_{i k}=\mathbb{E}\left[\phi\left(f_{l-1}(\mathbf{x}), y_{i}^{[l-1]} ; \theta_{l y}\right) \phi\left(f_{l-1}(\mathbf{x}), y_{k}^{[l-1]} ; \theta_{l y}\right) \mid \mathbf{Y}_{l}\right]$. The closed-form expressions of $\xi_{i}$ and $\zeta_{i k}$ are provided in the following subsections.

## S2.1 Matérn-1.5 kernel

For Matérn-1.5 kernel, $\Phi_{l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\prod_{j=1}^{d} \phi\left(x_{j}, x_{j}^{\prime} ; \theta_{l j}\right)=\prod_{j=1}^{d}\left(1+\frac{\sqrt{3}\left|x_{j}-x_{j}^{\prime}\right|}{\theta_{l j}}\right) \exp \left(-\frac{\sqrt{3}\left|x_{j}-x_{j}^{\prime}\right|}{\theta_{l j}}\right)$, $\xi_{i}$ and $\zeta_{i k}$ are provided as follows,

$$
\begin{aligned}
& \xi_{i}=\exp \left(\frac{3 \sigma_{l-1}^{* 2}(\mathbf{x})+2 \sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)}{2 \theta_{l y}^{2}}\right) \times\left[E_{1}^{\prime} \Lambda_{11} \Psi\left(\frac{\mu_{l-1}^{*}(\mathbf{x})-y_{i}^{[l-1]}-\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}{\sigma_{l-1}^{*}(\mathbf{x})}\right)\right. \\
& \left.+E_{1}^{\prime} \Lambda_{12} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})+\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& +\exp \left(\frac{3 \sigma_{l-1}^{* 2}(\mathbf{x})-2 \sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)}{2 \theta_{l y}^{2}}\right) \times\left[E_{2}^{\prime} \Lambda_{21} \Psi\left(\frac{-\mu_{l-1}^{*}(\mathbf{x})+y_{i}^{[l-1]}-\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}{\sigma_{l-1}^{*}(\mathbf{x})}\right)\right. \\
& \left.+E_{2}^{\prime} \Lambda_{12} \cdot \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& \zeta_{i k}=\exp \left\{\frac{6 \sigma_{l-1}^{* 2}(\mathbf{x})+\sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}-2 \mu_{l-1}^{*}(\mathbf{x})\right)}{\theta_{l y}^{2}}\right\} \\
& \times\left[E_{3}^{\prime} \Lambda_{31} \Psi\left\{\frac{\left(\mu_{l-1}^{*}(\mathbf{x})-y_{k}^{[l-1]}-2 \sqrt{3} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right. \\
& \left.+E_{3}^{\prime} \Lambda_{32} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{k}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})+2 \sqrt{3} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right]+\exp \left\{-\frac{\sqrt{3}\left(y_{k}^{[l-1]}-y_{i}^{[l-1]}\right)}{\theta_{l y}}\right\} \\
& \times\left[E_{4}^{\prime} \Lambda_{41}\left(\Psi\left\{\frac{y_{k}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}-\Psi\left\{\frac{y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right)\right. \\
& \left.+E_{4}^{\prime} \Lambda_{42} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)-E_{4}^{\prime} \Lambda_{43} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{k}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& +\exp \left\{\frac{6 \sigma_{l-1}^{* 2}(\mathbf{x})-\sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}-2 \mu_{l-1}^{*}(\mathbf{x})\right)}{\theta_{l y}^{2}}\right\} \\
& \times\left[E_{5}^{\prime} \Lambda_{51} \Psi\left\{\frac{\left(-\mu_{l-1}^{*}(\mathbf{x})+y_{k}^{[l-1]}-2 \sqrt{3} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right.
\end{aligned}
$$

$$
\begin{aligned}
& \left.+E_{5}^{\prime} \Lambda_{52} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})-2 \sqrt{3} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& \Lambda_{11}=\left(\begin{array}{c}
1 \\
\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}
\end{array}\right), \Lambda_{12}=\left(\begin{array}{l}
0 \\
1
\end{array}\right), \Lambda_{21}=\left(\begin{array}{c}
1 \\
-\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}
\end{array}\right) \\
& \Lambda_{31}=\left(\begin{array}{c}
1 \\
\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
\left(\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}+\sigma_{l-1}^{* 2}(\mathbf{x})
\end{array}\right), \Lambda_{32}=\left(\begin{array}{c}
0 \\
1 \\
\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}+y_{k}^{[l-1]}
\end{array}\right) \\
& \Lambda_{41}=\left(\begin{array}{c}
1 \\
\mu_{l-1}^{*}(\mathbf{x}) \\
\left(\mu_{l-1}^{*}(\mathbf{x})\right)^{2}+\sigma_{l-1}^{* 2}(\mathbf{x})
\end{array}\right), \Lambda_{42}=\left(\begin{array}{c}
0 \\
1 \\
\mu_{l-1}^{*}(\mathbf{x})+y_{i}^{[l-1]}
\end{array}\right) \\
& \Lambda_{43}=\left(\begin{array}{c}
0 \\
1 \\
\mu_{l-1}^{*}(\mathbf{x})+y_{k}^{[l-1]}
\end{array}\right), \Lambda_{51}=\left(\begin{array}{c}
1 \\
-\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
\left(-\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}+\sigma_{l-1}^{* 2}(\mathbf{x})
\end{array}\right) \\
& \Lambda_{52}=\left(\begin{array}{c}
0 \\
1 \\
-\mu_{l-1}^{*}(\mathbf{x})-\frac{2 \sqrt{3} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}-y_{i}^{[l-1]}
\end{array}\right), E_{1}=\frac{1}{\theta_{l y}}\left(\begin{array}{c}
\theta_{l y}-\sqrt{3} y_{i}^{[l-1]} \\
\sqrt{3}
\end{array}\right) \\
& E_{2}=\frac{1}{\theta_{l y}}\left(\begin{array}{c}
\theta_{l y}+\sqrt{3} y_{i}^{[l-1]} \\
\sqrt{3}
\end{array}\right), E_{3}=\frac{1}{\theta_{l y}^{2}}\left(\begin{array}{c}
\theta_{l y}^{2}+3 y_{i}^{[l-1]} y_{k}^{[l-1]}-\sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
2 \sqrt{3} \theta_{l y}-3\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
3
\end{array}\right) \\
& E_{4}=\frac{1}{\theta_{l y}^{2}}\left(\begin{array}{c}
\theta_{l y}^{2}-3 y_{i}^{[l-1]} y_{k}^{[l-1]}+\sqrt{3} \theta_{l y}\left(y_{k}^{[l-1]}-y_{i}^{[l-1]}\right) \\
3\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
-3
\end{array}\right) \\
& E_{5}=\frac{1}{\theta_{l y}^{2}}\left(\begin{array}{c}
\theta_{l y}^{2}+3 y_{i}^{[l-1]} y_{k}^{[l-1]}+\sqrt{3} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
2 \sqrt{3} \theta_{l y}+3\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
3
\end{array}\right)
\end{aligned}
$$

for $y_{i}^{[l-1]} \leq y_{k}^{[l-1]}$. If $y_{i}^{[l-1]}>y_{k}^{[l-1]}$, interchange $y_{i}^{[l-1]}$ and $y_{k}^{[l-1]} . \Psi$ is the cumulative distribution function of a standard normal distribution.

## S2.2 Matérn-2.5 kernel

For Matérn-2.5 kernel $\Phi_{l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\prod_{j=1}^{d}\left(1+\frac{\sqrt{5}\left|x_{j}-x_{j}^{\prime}\right|}{\theta_{l j}}+\frac{5\left(x_{j}-x_{j}^{\prime}\right)^{2}}{3 \theta_{l j}^{2}}\right) \exp \left(-\frac{\sqrt{5}\left|x_{j}-x_{j}^{\prime}\right|}{\theta_{l j}}\right), \xi_{i}$ and $\zeta_{i k}$ are provided as follows,

$$
\begin{aligned}
\xi_{i} & =\exp \left(\frac{5 \sigma_{l-1}^{* 2}(\mathbf{x})+2 \sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)}{2 \theta_{l y}^{2}}\right) \times\left[E_{1}^{\prime} \Lambda_{11} \Psi\left(\frac{\mu_{l-1}^{*}(\mathbf{x})-y_{i}^{[l-1]}-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}{\sigma_{l-1}^{*}(\mathbf{x})}\right)\right. \\
& \left.+E_{1}^{\prime} \Lambda_{12} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left.\left.\mu_{l-1}^{*}(\mathbf{x})-y_{i}^{[l-1]}-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}\right)}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& +\exp \left(\frac{5 \sigma_{l-1}^{* 2}(\mathbf{x})-2 \sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)}{2 \theta_{l y}^{2}}\right) \times\left[E_{2}^{\prime} \Lambda_{21} \Psi\left(\frac{-\mu_{l-1}^{*}(\mathbf{x})+y_{i}^{[l-1]}-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}}{\sigma_{l-1}^{*}(\mathbf{x})}\right)\right. \\
& \left.+E_{2}^{\prime} \Lambda_{22} \cdot \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(-\mu_{l-1}^{*}(\mathbf{x})+y_{i}^{[l-1]}-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
\zeta_{i k} & =\exp \left\{\frac{10 \sigma_{l-1}^{* 2}(\mathbf{x})+\sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}-2 \mu_{l-1}^{*}(\mathbf{x})\right)}{\theta_{l y}^{2}}\right\} \\
& \times\left[E_{3}^{\prime} \Lambda_{31} \Psi\left\{\frac{\left(\mu_{l-1}^{*}(\mathbf{x})-y_{k}^{[l-1]}-2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right. \\
& \left.+E_{3}^{\prime} \Lambda_{32} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(\mu_{l-1}^{*}(\mathbf{x})-y_{k}^{[l-1]}-2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right]+\exp \left\{-\frac{\sqrt{5}\left(y_{k}^{[l-1]}-y_{i}^{[l-1]}\right)}{\theta_{l y}}\right\} \\
& \times\left[E_{4}^{\prime} \Lambda_{41}\left(\Psi\left\{\frac{y_{k}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}-\Psi\left\{\frac{y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right)\right. \\
& \left.+E_{4}^{\prime} \Lambda_{42} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}^{[l-1]}-\mu_{l-1}^{*}(\mathbf{x})\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)-E_{4}^{\prime} \Lambda_{43} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{k}^{[l-1]}-\mu_{t-1}^{*}(\mathbf{x})\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right]
\end{aligned}
$$

$$
\begin{aligned}
& +\exp \left\{\frac{10 \sigma_{l-1}^{* 2}(\mathbf{x})-\sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}-2 \mu_{l-1}^{*}(\mathbf{x})\right)}{\theta_{l y}^{2}}\right\} \\
& \times\left[E_{5}^{\prime} \Lambda_{51} \Psi\left\{\frac{\left(-\mu_{l-1}^{*}(\mathbf{x})+y_{k}^{[l-1]}-2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)}{\sigma_{l-1}^{*}(\mathbf{x})}\right\}\right. \\
& \left.+E_{5}^{\prime} \Lambda_{52} \frac{\sigma_{l-1}^{*}(\mathbf{x})}{\sqrt{2 \pi}} \exp \left(-\frac{\left(-\mu_{l-1}^{*}(\mathbf{x})+y_{i}^{[l-1]}-2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}}{2 \sigma_{l-1}^{* 2}(\mathbf{x})}\right)\right] \\
& \Lambda_{11}=\left(\begin{array}{c}
1 \\
\mu_{t-1}^{*}(\mathbf{x})-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
\left(\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}+\sigma_{l-1}^{* 2}(\mathbf{x})
\end{array}\right), \Lambda_{12}=\left(\begin{array}{c}
0 \\
1 \\
\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}+y_{i}^{[l-1]}
\end{array}\right) \\
& \Lambda_{21}=\left(\begin{array}{c}
1 \\
-\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
\left(\mu_{l-1}^{*}(\mathbf{x})+\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}\right)^{2}+y_{i}^{[l-1]}
\end{array}\right), \Lambda_{22}=\left(\begin{array}{c}
0 \\
1 \\
-\mu_{l-1}^{*}(\mathbf{x})-\frac{\sqrt{5} \sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}}-y_{i}^{[l-1]}
\end{array}\right) \\
& \Lambda_{31}=\left(\begin{array}{c}
1 \\
\mu_{c} \\
\mu_{c}^{2}+\sigma_{l-1}^{* 2}(\mathbf{x}) \\
\mu_{c}\left(\mu_{c}^{2}+3 \sigma_{l-1}^{* 2}(\mathbf{x})\right) \\
\mu_{c}^{4}+6 \mu_{c}^{2} \sigma_{l-1}^{* 2}(\mathbf{x})+3 \sigma_{l-1}^{* 4}(\mathbf{x})
\end{array}\right), \mu_{c}=\mu_{l-1}^{*}(\mathbf{x})-2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
& \Lambda_{32}=\left(\begin{array}{c}
0 \\
1 \\
\mu_{c}+y_{k}^{[l-1]} \\
\mu_{c}^{2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})+\left(y_{k}^{[l-1]}\right)^{2}+\mu_{c} y_{k}^{[l-1]} \\
\mu_{c}^{3}+\left(y_{k}^{[l-1]}\right)^{3}+y_{k}^{[l-1]} \mu_{c}\left(\mu_{c}+y_{k}^{[l-1]}\right)+\sigma_{l-1}^{* 2}(\mathbf{x})\left(5 \mu_{c}+3 y_{k}^{[l-1]}\right)
\end{array}\right)
\end{aligned}
$$

$$
\begin{aligned}
& \Lambda_{41}=\left(\begin{array}{c}
1 \\
\mu^{*} \\
\mu^{* 2}+\sigma_{l-1}^{* 2}(\mathbf{x}) \\
\mu^{*}\left(\mu^{* 2}+3 \sigma_{l-1}^{* 2}(\mathbf{x})\right) \\
\mu^{* 4}+6 \mu_{c}^{2} \sigma_{l-1}^{* 2}(\mathbf{x})+3 \sigma_{l-1}^{* 4}(\mathbf{x})
\end{array}\right), \mu^{*}=\mu_{l-1}^{*}(\mathbf{x}) \\
& \Lambda_{42}=\left(\begin{array}{c}
0 \\
1 \\
\mu^{*}+y_{i}^{[l-1]} \\
\mu^{* 2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})+\left(y_{i}^{[l-1]}\right)^{2}+\mu^{*} y_{i}^{[l-1]} \\
\mu^{* 3}+\left(y_{i}^{[l-1]}\right)^{3}+y_{i}^{[l-1]} \mu^{*}\left(\mu^{*}+y_{i}^{[l-1]}\right)+\sigma_{l-1}^{* 2}(\mathbf{x})\left(5 \mu^{*}+3 y_{i}^{[l-1]}\right)
\end{array}\right) \\
& \Lambda_{43}=\left(\begin{array}{c}
0 \\
1 \\
\mu^{*}+y_{k}^{[l-1]} \\
\mu^{* 2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})+\left(y_{k}^{[l-1]}\right)^{2}+\mu^{*} y_{k}^{[l-1]} \\
\mu^{* 3}+\left(y_{k}^{[l-1]}\right)^{3}+y_{k}^{[l-1]} \mu^{*}\left(\mu^{*}+y_{k}^{[l-1]}\right)+\sigma_{l-1}^{* 2}(\mathbf{x})\left(5 \mu^{*}+3 y_{k}^{[l-1]}\right)
\end{array}\right) \\
& \Lambda_{51}=\left(\begin{array}{c}
1 \\
-\mu_{d} \\
\mu_{d}^{2}+\sigma_{l-1}^{* 2}(\mathbf{x}) \\
-\mu_{d}\left(\mu_{d}^{2}+3 \sigma_{l-1}^{* 2}(\mathbf{x})\right) \\
\mu_{d}^{4}+6 \mu_{d}^{2} \sigma_{l-1}^{* 2}(\mathbf{x})+3 \sigma_{l-1}^{* 4}(\mathbf{x})
\end{array}\right), \mu_{d}=\mu_{l-1}^{*}(\mathbf{x})+2 \sqrt{5} \frac{\sigma_{l-1}^{* 2}(\mathbf{x})}{\theta_{l y}} \\
& \Lambda_{52}=\left(\begin{array}{c}
0 \\
1 \\
-\mu_{d}-y_{i}^{[l-1]} \\
\mu_{d}^{2}+2 \sigma_{l-1}^{* 2}(\mathbf{x})+\left(y_{i}^{[l-1]}\right)^{2}+\mu_{d} y_{i}^{[l-1]} \\
-\mu_{d}^{3}-\left(y_{i}^{[l-1]}\right)^{3}-y_{i}^{[l-1]} \mu_{d}\left(\mu_{d}+y_{i}^{[l-1]}\right)-\sigma_{l-1}^{* 2}(\mathbf{x})\left(5 \mu_{d}+3 y_{i}^{[l-1]}\right)
\end{array}\right)
\end{aligned}
$$

$$
\begin{aligned}
& E_{1}=\frac{1}{3 \theta_{l y}^{2}}\left(\begin{array}{c}
3 \theta_{l y}^{2}-3 \sqrt{5} \theta_{l y} y_{i}^{[l-1]}+5\left(y_{i}^{[l-1]}\right)^{2} \\
3 \sqrt{5} \theta_{l y}-10 y_{i}^{[l-1]} \\
5
\end{array}\right), E_{2}=\frac{1}{3 \theta_{l y}^{2}}\left(\begin{array}{c}
3 \theta_{l y}^{2}+3 \sqrt{5} \theta_{l y} y_{i}^{[l-1]}+5\left(y_{i}^{[l-1]}\right)^{2} \\
3 \sqrt{5} \theta_{l y}+10 y_{i}^{[l-1]} \\
5
\end{array}\right) \\
& E_{3}=\frac{1}{9 \theta_{l y}^{4}}\left(\begin{array}{lllll}
E_{31} & E_{32} & E_{33} & E_{34} & E_{35}
\end{array}\right)^{\top} \\
& E_{31}=9 \theta_{l y}^{4}+25\left(y_{i}^{[l-1]}\right)^{2}\left(y_{k}^{[l-1]}\right)^{2}-3 \sqrt{5} \theta_{l y}\left(3 \theta_{l y}^{2}+5 y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
& +15 \theta_{l y}^{2}\left(\left(y_{i}^{[l-1]}\right)^{2}+\left(y_{k}^{[l-1]}\right)^{2}+3 y_{i}^{[l-1]} y_{k}^{[l-1]}\right) \\
& E_{32}=18 \sqrt{5} \theta_{l y}^{3}+15 \sqrt{5} \theta_{l y}\left(\left(y_{i}^{[l-1]}\right)^{2}+\left(y_{k}^{[l-1]}\right)^{2}\right)-75 \theta_{l y}^{2}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
& -50 y_{i}^{[l-1]} y_{k}^{[l-1]}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)+60 \sqrt{5} \theta_{l y} y_{i}^{[l-1]} y_{k}^{[l-1]} \\
& E_{33}=5\left\{5\left(y_{i}^{[l-1]}\right)^{2}+5\left(y_{k}^{[l-1]}\right)^{2}+15 \theta_{l y}^{2}-9 \sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)+20\left(y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\right\} \\
& E_{34}=10\left(3 \sqrt{5} \theta_{l y}-5 y_{i}^{[l-1]}-5 y_{k}^{[l-1]}\right), E_{35}=25 \\
& E_{4}=\frac{1}{9 \theta_{l y}^{4}}\left(\begin{array}{lllll}
E_{41} & E_{42} & E_{43} & E_{44} & E_{45}
\end{array}\right)^{\top} \\
& E_{41}=9 \theta_{l y}^{4}+25\left(y_{i}^{[l-1]}\right)^{2}\left(y_{k}^{[l-1]}\right)^{2}+3 \sqrt{5} \theta_{l y}\left(3 \theta_{l y}^{2}-5 y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\left(y_{i}^{[l-1]}-y_{k}^{[l-1]}\right) \\
& +15 \theta_{l y}^{2}\left(\left(y_{i}^{[l-1]}\right)^{2}+\left(y_{k}^{[l-1]}\right)^{2}-3 y_{i}^{[l-1]} y_{k}^{[l-1]}\right) \\
& E_{42}=5\left\{3 \sqrt{5} \theta_{l y}\left(\left(y_{k}^{[l-1]}\right)^{2}-\left(y_{i}^{[l-1]}\right)^{2}\right)+3 \theta_{l y}^{2}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)-10 y_{i}^{[l-1]} y_{k}^{[l-1]}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)\right\} \\
& E_{43}=5\left\{5\left(y_{i}^{[l-1]}\right)^{2}+5\left(y_{k}^{[l-1]}\right)^{2}-3 \theta_{l y}^{2}-3 \sqrt{5} \theta_{l y}\left(y_{k}^{[l-1]}-y_{i}^{[l-1]}\right)+20\left(y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\right\} \\
& E_{44}=-50\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right), E_{45}=25 \\
& E_{5}=\frac{1}{9 \theta_{l y}^{4}}\left(\begin{array}{lllll}
E_{51} & E_{52} & E_{53} & E_{54} & E_{55}
\end{array}\right)^{\top} \\
& E_{51}=9 \theta_{l y}^{4}+25\left(y_{i}^{[l-1]}\right)^{2}\left(y_{k}^{[l-1]}\right)^{2}+3 \sqrt{5} \theta_{l y}\left(3 \theta_{l y}^{2}+5 y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
& +15 \theta_{l y}^{2}\left(\left(y_{i}^{[l-1]}\right)^{2}+\left(y_{k}^{[l-1]}\right)^{2}+3 y_{i}^{[l-1]} y_{k}^{[l-1]}\right) \\
& E_{52}=18 \sqrt{5} \theta_{l y}^{3}+15 \sqrt{5} \theta_{l y}\left(\left(y_{i}^{[l-1]}\right)^{2}+\left(y_{k}^{[l-1]}\right)^{2}\right)+75 \theta_{l y}^{2}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right) \\
& +50 y_{i}^{[l-1]} y_{k}^{[l-1]}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)+60 \sqrt{5} \theta_{l y} y_{i}^{[l-1]} y_{k}^{[l-1]}
\end{aligned}
$$

$$
\begin{aligned}
& E_{53}=5\left\{5\left(y_{i}^{[l-1]}\right)^{2}+5\left(y_{k}^{[l-1]}\right)^{2}+15 \theta_{l y}^{2}+9 \sqrt{5} \theta_{l y}\left(y_{i}^{[l-1]}+y_{k}^{[l-1]}\right)+20\left(y_{i}^{[l-1]} y_{k}^{[l-1]}\right)\right\} \\
& E_{54}=10\left(3 \sqrt{5} \theta_{l y}+5 y_{i}^{[l-1]}+5 y_{k}^{[l-1]}\right), E_{55}=25
\end{aligned}
$$

for $y_{i}^{[l-1]} \leq y_{k}^{[l-1]}$. If $y_{i}^{[l-1]}>y_{k}^{[l-1]}$, interchange $y_{i}^{[l-1]}$ and $y_{k}^{[l-1]}$.


[^0]:    *These authors gratefully acknowledge funding from NSF DMS 2113407.

